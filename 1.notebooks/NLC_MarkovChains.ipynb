{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQbkSYL8DhPE"
   },
   "source": [
    "## Our Development Philosophy: From Baseline to Optimized\n",
    "\n",
    "This project follows a deliberate and iterative approach to model development. Our core principle is to ensure that every change is measured, understood, and contributes positively to the final result. This methodology transforms the development process from a series of guesses into a scientific experiment.\n",
    "\n",
    "### Phase 1: Build the Brute-Force Baseline\n",
    "\n",
    "First, we create the simplest possible, end-to-end working model. This **bare-minimum** version is not intended to be sophisticated; its purpose is to be a functional starting point that serves two critical purposes:\n",
    "\n",
    "1.  **Proof of Concept:** It confirms that our data pipeline, from loading to processing and modeling, is functional. It validates our foundational assumptions.\n",
    "2.  **Establish a Benchmark:** It provides a clear **baseline performance metric**. All future work will be measured against this initial score. Without a baseline, it's impossible to quantify improvement.\n",
    "\n",
    "### Phase 2: Optimize One Step at a Time\n",
    "\n",
    "Once the baseline is established, we begin a cycle of incremental improvement. We strictly adhere to the principle of making **one isolated change at a time**. This is the most critical aspect of our philosophy.\n",
    "\n",
    "Instead of overhauling the model at once and being unable to pinpoint the source of a change, we will:\n",
    "\n",
    "* **Target** a single component for improvement (e.g., memory usage, context awareness, grammar).\n",
    "* **Implement** that one specific change.\n",
    "* **Measure** its exact impact on performance and resource consumption.\n",
    "\n",
    "This methodical process removes guesswork and allows us to attribute performance gains or losses directly to a specific action, ensuring that every step forward is a confident one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading the Dataset\n",
    "\n",
    "Our journey begins by loading the `wikitext` dataset, a large corpus of text derived from high-quality Wikipedia articles. We use the `datasets` library from Hugging Face for this task, which provides a standardized and efficient way to handle large datasets.\n",
    "\n",
    "We also specify a custom cache directory (`../2.data`) to ensure that the downloaded data is stored in a predictable location within our project structure, making it easier to manage and reuse without re-downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSS3p4etDy-C",
    "outputId": "8da2a36b-12c7-4301-a702-efcf9010f430"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jamin Carter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define a custom directory to cache the downloaded dataset\n",
    "my_custom_cache_dir = \"../2.data\"\n",
    "\n",
    "# Load the 'wikitext-2-raw-v1' version of the wikitext dataset\n",
    "wikitext_dataset = load_dataset(\n",
    "    \"wikitext\",\n",
    "    \"wikitext-2-raw-v1\",\n",
    "    cache_dir=my_custom_cache_dir\n",
    ")\n",
    "\n",
    "# Print the dataset structure to see the different splits (train, validation, test)\n",
    "print(wikitext_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initial Data Exploration\n",
    "\n",
    "Before we can clean the data, we need to understand its raw form. We'll inspect the first few lines of the training set to identify patterns, noise, and artifacts that need to be addressed in our preprocessing stage. This step is crucial for formulating an effective cleaning strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7eMZxqOFVyA",
    "outputId": "0ec87883-ac4a-4b1e-9fff-fbe96156146f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\n",
      "1: = Valkyria Chronicles III = \n",
      "\n",
      "2:\n",
      "3: Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      "\n",
      "4: The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create separate variables for easier access to each dataset split\n",
    "test_data = wikitext_dataset['test']\n",
    "train_data = wikitext_dataset['train']\n",
    "validation_data = wikitext_dataset['validation']\n",
    "\n",
    "# Print the first 5 lines of the training data to inspect its content\n",
    "for i in range(5):\n",
    "  print(f\"{i}:{train_data[i]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKOkr78xHxzu"
   },
   "source": [
    "### Step 3: Defining the Preprocessing Strategy\n",
    "\n",
    "Based on our initial exploration, we have identified several types of noise in the raw text. To prepare the data for our model, we will implement a multi-step cleaning process:\n",
    "\n",
    "* **Remove Article Headings:** The dataset uses lines starting with ` = ` to denote section titles (e.g., `= = Gameplay = =`). These are metadata, not narrative text, and should be removed.\n",
    "* **Handle Special Characters:** The text contains artifacts like `@-@` and non-ASCII characters (e.g., `戦場のヴァルキュリア3`). We will remove these to create a cleaner, more consistent vocabulary.\n",
    "* **Standardize Casing:** All text will be converted to lowercase to ensure that the model treats the same word (e.g., \"The\" and \"the\") as a single entity.\n",
    "* **Remove Empty Lines:** The dataset contains many blank lines, which add no value and should be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Measuring the Impact of Preprocessing\n",
    "\n",
    "Before applying our cleaning functions, we will establish a baseline measurement of the dataset's size. We'll calculate the total number of rows and characters in each split (train, validation, and test). After preprocessing, we will perform the same calculation again. This allows us to quantify the exact impact of our cleaning process, showing how much noise we have successfully removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "li5oyf7PGZ47",
    "outputId": "fd6be37e-0789-496a-d35d-96bd6135b833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before Cleaning ---\n",
      "Training Data:   36,718 rows, 10,892,990 characters\n",
      "Validation Data: 3,760 rows, 1,142,150 characters\n",
      "Test Data:       4,358 rows, 1,285,622 characters\n",
      "\n",
      "--- After Cleaning ---\n",
      "Training Data:   23,764 rows, 10,617,709 characters\n",
      "Validation Data: 2,461 rows, 1,114,644 characters\n",
      "Test Data:       2,891 rows, 1,253,140 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# --- Before Preprocessing ---\n",
    "print(\"--- Before Cleaning ---\")\n",
    "# Calculate and print the initial size of each dataset split\n",
    "train_chars_before = sum(len(line) for line in train_data['text'] if line)\n",
    "validation_chars_before = sum(len(line) for line in validation_data['text'] if line)\n",
    "test_chars_before = sum(len(line) for line in test_data['text'] if line)\n",
    "print(f\"Training Data:   {train_data.num_rows:,} rows, {train_chars_before:,} characters\")\n",
    "print(f\"Validation Data: {validation_data.num_rows:,} rows, {validation_chars_before:,} characters\")\n",
    "print(f\"Test Data:       {test_data.num_rows:,} rows, {test_chars_before:,} characters\")\n",
    "\n",
    "# --- Preprocessing Steps ---\n",
    "# The 'datasets' library allows us to chain operations for a clean pipeline.\n",
    "\n",
    "# 1. Filter out article headings\n",
    "processed_dataset = wikitext_dataset.filter(\n",
    "    lambda example: not example['text'].strip().startswith(' = ')\n",
    ")\n",
    "\n",
    "# 2. Apply text cleaning and normalization to each entry\n",
    "processed_dataset = processed_dataset.map(\n",
    "    lambda example: {\n",
    "        'text': re.sub(\n",
    "            r'[^a-zA-Z0-9\\s.,\\'?!-]', '', # Keep only alphanumeric, common punctuation, and whitespace\n",
    "            example['text'].lower().replace('@-@', '') # Convert to lowercase and remove artifacts\n",
    "        ).strip()\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Filter out any lines that became empty after cleaning\n",
    "processed_dataset = processed_dataset.filter(\n",
    "    lambda example: len(example['text']) > 0\n",
    ")\n",
    "\n",
    "# --- After Preprocessing ---\n",
    "print(\"\\n--- After Cleaning ---\")\n",
    "# Re-assign the cleaned data to our variables for future use\n",
    "train_data = processed_dataset['train']\n",
    "validation_data = processed_dataset['validation']\n",
    "test_data = processed_dataset['test']\n",
    "\n",
    "# Calculate and print the final size of each dataset split\n",
    "train_chars_after = sum(len(line) for line in train_data['text'] if line)\n",
    "validation_chars_after = sum(len(line) for line in validation_data['text'] if line)\n",
    "test_chars_after = sum(len(line) for line in test_data['text'] if line)\n",
    "print(f\"Training Data:   {train_data.num_rows:,} rows, {train_chars_after:,} characters\")\n",
    "print(f\"Validation Data: {validation_data.num_rows:,} rows, {validation_chars_after:,} characters\")\n",
    "print(f\"Test Data:       {test_data.num_rows:,} rows, {test_chars_after:,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Verifying the Cleaned Data\n",
    "\n",
    "After preprocessing, it's essential to visually inspect the data again. This final check confirms that our cleaning functions worked as expected and that the text is now in a suitable format for our model. We expect to see clean, lowercase text with no strange artifacts or article headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCH8m9xVK52I",
    "outputId": "04824f3d-eeb7-4240-ab41-1aba1c299fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:valkyria chronicles iii\n",
      "1:senj no valkyria 3  unrecorded chronicles  japanese  3 , lit . valkyria of the battlefield 3  , commonly referred to as valkyria chronicles iii outside japan , is a tactical role  playing video game developed by sega and media.vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . employing the same fusion of tactical and real  time gameplay as its predecessors , the story runs parallel to the first game and follows the  nameless  , a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit  calamaty raven  .\n",
      "2:the game began development in 2010 , carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . character designer raita honjou and composer hitoshi sakimoto both returned from previous entries , along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game 's opening theme was sung by may 'n .\n",
      "3:it met with positive sales in japan , and was praised by both japanese and western critics . after release , it received downloadable content , along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . media.vision would return to the franchise with the development of valkyria  azure revolution for the playstation 4 .\n",
      "4:gameplay\n"
     ]
    }
   ],
   "source": [
    "# Visually inspect the first 20 lines of the cleaned training data\n",
    "for i in range(5):\n",
    "  print(f\"{i}:{train_data[i]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIJMdwzPN5_P"
   },
   "source": [
    "## Baseline Model: The Simple Markov Chain\n",
    "\n",
    "Our first model is a foundational, first-order Markov chain. It's a simple probabilistic model that predicts the next word in a sequence based *solely* on the current word, ignoring all previous context.\n",
    "\n",
    "### What is \"The Model\"?\n",
    "\n",
    "In this context, the \"model\" isn't a complex algorithm but a simple and intuitive data structure: a **Python dictionary**. This dictionary is the final output of the \"training\" process. It stores all the learned word-to-word transition probabilities from the input text.\n",
    "\n",
    "### The Structure of the Model (The Dictionary)\n",
    "\n",
    "The model is a nested dictionary with a specific structure: `word -> {next_word: count}`.\n",
    "\n",
    "* **Level 1: The Keys (Current Words)**\n",
    "    * The keys of the main dictionary are all the unique words found in the text. Each key represents a possible \"current state.\"\n",
    "* **Level 2: The Values (Next Words and Their Frequencies)**\n",
    "    * The value associated with each key is *another dictionary*.\n",
    "    * In this inner dictionary, the keys are all the words that have ever appeared immediately after the \"current word.\"\n",
    "    * The values are the counts (integers) of how many times that specific transition occurred.\n",
    "\n",
    "### A Concrete Example\n",
    "\n",
    "If our training text is: `\"the cat sat on the mat\"`\n",
    "\n",
    "The resulting model dictionary would look like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"the\": {\n",
    "        \"cat\": 1,\n",
    "        \"mat\": 1\n",
    "    },\n",
    "    \"cat\": {\n",
    "        \"sat\": 1\n",
    "    },\n",
    "    \"sat\": {\n",
    "        \"on\": 1\n",
    "    },\n",
    "    \"on\": {\n",
    "        \"the\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDXONapyNFhX",
    "outputId": "82e77f97-aa74-4760-bc30-befeb8e9b3c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training corpus...\n",
      "Corpus prepared.\n",
      "Building Markov model...\n",
      "Loading saved model...\n",
      "\n",
      "--- Text Generated from WikiText Model ---\n",
      "morning of a period can identify who saw a week later applied to read and peter finch explains that nicole is somewhat difficult to show the highway between djedkare isesi . picasso , while on 5 , they instruct individuals . carre 's 1130 , native birds . u during the game characters compete in an interview for a council was in 1 . consequently requests the impression along many different from mile 1 in\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import json\n",
    "\n",
    "\n",
    "def build_markov_model(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Builds a simple Markov chain model from a given text.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "\n",
    "    model = {}\n",
    "\n",
    "    for i in range(len(tokens) - 1):\n",
    "        current_word = tokens[i]\n",
    "        next_word = tokens[i + 1]\n",
    "\n",
    "        if current_word not in model:\n",
    "            model[current_word] = {}\n",
    "\n",
    "        if next_word not in model[current_word]:\n",
    "            model[current_word][next_word] = 0\n",
    "\n",
    "        model[current_word][next_word] += 1\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_text(model: dict, length: int = 50,start: str=None) -> str:\n",
    "    if start is None:\n",
    "      start_word=random.choice(list(model.keys()))\n",
    "    else:\n",
    "        start_word=start.lower()\n",
    "    \"\"\"\n",
    "    Generates new text using a pre-built Markov model.\n",
    "    \"\"\"\n",
    "    generated_text = [start_word]\n",
    "    current_word = start_word\n",
    "\n",
    "    for _ in range(length - 1):\n",
    "        if current_word not in model:\n",
    "            break\n",
    "\n",
    "        next_words_dict = model[current_word]\n",
    "        possible_next_words = list(next_words_dict.keys())\n",
    "        word_frequencies = list(next_words_dict.values())\n",
    "\n",
    "        chosen_next_word = random.choices(possible_next_words, weights=word_frequencies, k=1)[0]\n",
    "\n",
    "        generated_text.append(chosen_next_word)\n",
    "        current_word = chosen_next_word\n",
    "\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "\n",
    "# Helper Functions Ignore them\n",
    "import os\n",
    "model_filepath = \"../3.model/Markov_chain_v0.1\" # <-- Change this path\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    \"\"\"Saves the model dictionary to a JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(model, f)\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"Loads the model dictionary from a JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# --- Main execution ---\n",
    "\n",
    "# 1. Convert the 'train' dataset object into a single corpus string\n",
    "# We join all the lines of text together with spaces in between.\n",
    "print(\"Preparing training corpus...\")\n",
    "corpus = \" \".join(train_data['text'])\n",
    "print(\"Corpus prepared.\")\n",
    "\n",
    "# 2. Build the model using the training corpus\n",
    "print(\"Building Markov model...\")\n",
    "if os.path.exists(model_filepath):\n",
    "    print(\"Loading saved model...\")\n",
    "    markov_model = load_model(model_filepath)\n",
    "else:\n",
    "    print(\"No saved model found. Building a new one...\")\n",
    "    markov_model = build_markov_model(corpus)\n",
    "    save_model(markov_model, model_filepath)\n",
    "\n",
    "\n",
    "# 3. Generate new text from the trained model\n",
    "new_text = generate_text(markov_model, length=75,start=\"morning\")\n",
    "\n",
    "# 4. Print the result\n",
    "print(\"\\n--- Text Generated from WikiText Model ---\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5fri8tWSehW"
   },
   "source": [
    "## Optimization 1: Memory Efficiency\n",
    "\n",
    "The initial brute-force approach required loading the entire training dataset into a **single, massive string**. This is highly problematic for large datasets for two main reasons:\n",
    "\n",
    "1.  **High Memory Usage:** It can consume several gigabytes of RAM, potentially crashing the program with a `MemoryError` on systems with limited memory.\n",
    "2.  **Loss of Context:** It destroys natural sentence boundaries, leading to a lower-quality model that can't learn to start or end sentences properly.\n",
    "\n",
    "To solve this, we **process the data incrementally**. Instead of creating one large object in memory, the optimized approach is to build the model **line-by-line**.\n",
    "\n",
    "We iterate through the dataset, processing only one line at a time to update our model dictionary. This is a highly **memory-efficient** solution, as it allows us to train on a dataset of virtually any size while using a nearly constant amount of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcuTb1jaOoc3",
    "outputId": "12cf38a6-9990-4f90-a9f3-71d66816e3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model...\n",
      "Model is ready.\n",
      "\n",
      "--- Generating new text ---\n",
      "\n",
      "--- Generated Text ---\n",
      "morning assemblies called the foot prints and the age named ingleside and shades of saintly royalty until the sweet chariot , for six deaths . the brigade behind . from the bluebirds . the beak from 1855 . falling backwards onto smith notes that was killed in duluth , beautiful sight . meanwhile , sarah rodman of that winter storm damaged on quintana roo and a middle distance was very short period after each occasion\n"
     ]
    }
   ],
   "source": [
    "def build_markov_model_incrementally(dataset_split):\n",
    "    \"\"\"\n",
    "    Builds a model by iterating through the dataset line by line (memory-efficient).\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    for line in dataset_split['text']:\n",
    "        tokens = line.split()\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        for i in range(len(tokens) - 1):\n",
    "            current_word = tokens[i]\n",
    "            next_word = tokens[i + 1]\n",
    "\n",
    "            if current_word not in model:\n",
    "                model[current_word] = {}\n",
    "\n",
    "            if next_word not in model[current_word]:\n",
    "                model[current_word][next_word] = 0\n",
    "\n",
    "            model[current_word][next_word] += 1\n",
    "\n",
    "    return model\n",
    "# --- Main Execution (Incremental Approach) ---\n",
    "# Assumes 'train' dataset object and all helper functions are pre-defined.\n",
    "\n",
    "# 1. Configuration\n",
    "MODEL_FILEPATH = \"../3.model/Markov_chain_v0.2\"\n",
    "START_WORD = \"morning\"\n",
    "TEXT_LENGTH = 75\n",
    "\n",
    "# 2. Load or Build the Model Incrementally\n",
    "if os.path.exists(MODEL_FILEPATH):\n",
    "    print(\"Loading existing model...\")\n",
    "    markov_model = load_model(MODEL_FILEPATH)\n",
    "else:\n",
    "    print(\"No existing model found. Building a new one incrementally...\")\n",
    "\n",
    "    # This is the corrected, memory-efficient call.\n",
    "    # It passes the dataset object directly to the incremental builder.\n",
    "    markov_model = build_markov_model_incrementally(train_data)\n",
    "\n",
    "    save_model(markov_model, MODEL_FILEPATH)\n",
    "\n",
    "print(\"Model is ready.\")\n",
    "\n",
    "# 3. Generate and Print Text\n",
    "print(\"\\n--- Generating new text ---\")\n",
    "generated_text = generate_text(\n",
    "    markov_model,\n",
    "    length=TEXT_LENGTH,\n",
    "    start=START_WORD\n",
    ")\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9Ng2W3AUw3G"
   },
   "source": [
    "## Optimization 2: Increasing Context with a Larger Window\n",
    "\n",
    "The most significant weakness of our current model is its extremely limited memory. It is a **first-order Markov chain**, meaning it only knows the previous **1** word. This leads to a critical failure in logical consistency.\n",
    "\n",
    "### The Problem of Lost Context\n",
    "\n",
    "Consider the phrase: `\"The sky is...\"`\n",
    "\n",
    "Our model's decision for the next word is based *only* on the word `\"is\"`. It has forgotten the crucial context word `\"sky\"`. The model's pool of possible next words is created from *every single instance* where `\"is\"` appeared in the training text, regardless of what came before it. It learns from unrelated phrases like:\n",
    "\n",
    "* `\"...the sky is blue...\"`\n",
    "* `\"...the company's logo is an apple...\"`\n",
    "\n",
    "When the model's only context is `\"is\"`, it blends these unrelated scenarios. It might correctly identify `\"blue\"` as probable, but `\"apple\"` remains a possibility because the model has no memory of the preceding word, `\"sky\"`.\n",
    "\n",
    "### The Solution: Using Tuples for State\n",
    "\n",
    "To solve this, we increase the model's memory by using a **tuple of multiple words** as the key. This tuple acts as our new \"state,\" effectively filtering out irrelevant choices.\n",
    "\n",
    "* **Old Model State (Plain Word):** `is`\n",
    "    * Possible Next Words: `{ \"apple\": 3, \"blue\": 4, \"green\": 2, ... }`\n",
    "* **New Model State (Tuple):** `('sky', 'is')`\n",
    "    * Possible Next Words: `{ \"blue\": 150, \"clear\": 80, \"overcast\": 45, ... }`\n",
    "\n",
    "By using a tuple, the model's state is now `('sky', 'is')`. The pool of possible next words it considers is now completely different and far more relevant. The word `\"apple\"` is unlikely to even be in this new list, leading to much more coherent and context-aware text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RMaeZZYUIbO",
    "outputId": "ed703be6-226c-45b0-be06-569079049266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../3.model/Markov_chain_v0.3\\markov_model_ws2.json...\n",
      "Model loaded successfully.\n",
      "\n",
      "--- Text Generated from Windowed Model ---\n",
      "the sky or invisibly present within the icao definition and released in 2006 , darden pursued careers as a prospective tenant appears . mrs. o 'brien destroyer no. 55 dd 55 was laid on top of japanese emigration to brazil . maeda accepted gracie and others were creating . roger friedman of forbes called the scene of this tropical storm formed in april 1919 , but could not otherwise afford the player . he also\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import ast # Used to safely convert string representations of tuples back to tuples\n",
    "\n",
    "# (Assuming 'processed_dataset' is already created and split into train, validation, test)\n",
    "# train = processed_dataset['train']\n",
    "\n",
    "def build_markov_model_with_window(dataset_split, window_size=2):\n",
    "    \"\"\"\n",
    "    Builds a higher-order Markov model using a tuple of words (the \"window\") as the state.\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    for line in dataset_split['text']:\n",
    "        tokens = line.split()\n",
    "        if len(tokens) < window_size + 1:\n",
    "            continue\n",
    "        for i in range(len(tokens) - window_size):\n",
    "            current_state = tuple(tokens[i : i + window_size])\n",
    "            next_word = tokens[i + window_size]\n",
    "            if current_state not in model:\n",
    "                model[current_state] = {}\n",
    "            if next_word not in model[current_state]:\n",
    "                model[current_state][next_word] = 0\n",
    "            model[current_state][next_word] += 1\n",
    "    return model\n",
    "\n",
    "def save_model_window(model, filepath):\n",
    "    \"\"\"\n",
    "    Saves the model dictionary to a JSON file, converting tuple keys to strings.\n",
    "    \"\"\"\n",
    "    print(f\"Saving model to {filepath}...\")\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    # Convert tuple keys to strings because JSON does not support tuple keys\n",
    "    string_keyed_model = {str(key): value for key, value in model.items()}\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(string_keyed_model, f, indent=4)\n",
    "    print(\"Model saved successfully.\")\n",
    "\n",
    "def load_model_window(filepath):\n",
    "    \"\"\"\n",
    "    Loads the model from a JSON file, converting string keys back to tuples.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {filepath}...\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        string_keyed_model = json.load(f)\n",
    "        # Convert string keys back to tuples for the model to work correctly\n",
    "        model = {ast.literal_eval(key): value for key, value in string_keyed_model.items()}\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "def generate_text_with_window(model, length=75, window_size=2, start_seed=None):\n",
    "    \"\"\"\n",
    "    Generates text from a windowed model, optionally starting with a given seed phrase.\n",
    "    \"\"\"\n",
    "    start_state = None\n",
    "\n",
    "    # Try to use the provided start_seed\n",
    "    if start_seed:\n",
    "        seed_words = start_seed.lower().split()\n",
    "        # Validate that the seed has the correct number of words\n",
    "        if len(seed_words) != window_size:\n",
    "            print(f\"Warning: Your start seed has {len(seed_words)} words, but the model's window size is {window_size}. Starting randomly.\")\n",
    "        else:\n",
    "            potential_start_state = tuple(seed_words)\n",
    "            # Validate that the seed exists as a state in our model\n",
    "            if potential_start_state in model:\n",
    "                start_state = potential_start_state\n",
    "            else:\n",
    "                print(f\"Warning: The phrase {potential_start_state} was not found in the model's training data. Starting randomly.\")\n",
    "\n",
    "    # If no seed was provided or the provided seed was invalid, start randomly\n",
    "    if start_state is None:\n",
    "        start_state = random.choice(list(model.keys()))\n",
    "\n",
    "    # --- The rest of the function remains the same ---\n",
    "    generated_text = list(start_state)\n",
    "    current_state = start_state\n",
    "    for _ in range(length - window_size):\n",
    "        if current_state not in model:\n",
    "            break\n",
    "        next_words_dict = model[current_state]\n",
    "        possible_next_words = list(next_words_dict.keys())\n",
    "        word_frequencies = list(next_words_dict.values())\n",
    "        chosen_next_word = random.choices(possible_next_words, weights=word_frequencies, k=1)[0]\n",
    "        generated_text.append(chosen_next_word)\n",
    "        current_state = tuple(generated_text[-window_size:])\n",
    "\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# 1. Configuration\n",
    "WINDOW_SIZE = 2\n",
    "model_dir = \"../3.model/Markov_chain_v0.3\"\n",
    "# We'll make the filename dynamic based on the window size\n",
    "model_filename = f\"markov_model_ws{WINDOW_SIZE}.json\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "# 2. Load or Build the Model\n",
    "if os.path.exists(model_filepath):\n",
    "    windowed_markov_model = load_model_window(model_filepath)\n",
    "else:\n",
    "    print(\"Saved model not found. Training a new one...\")\n",
    "    windowed_markov_model = build_markov_model_with_window(train_data, window_size=WINDOW_SIZE)\n",
    "    save_model_window(windowed_markov_model, model_filepath)\n",
    "\n",
    "# 3. Generate new text from the loaded/trained model\n",
    "USER_START_SEED = \"the sky\"\n",
    "\n",
    "# This line has been updated to pass the start_seed to the function\n",
    "new_text = generate_text_with_window(\n",
    "    windowed_markov_model,\n",
    "    length=75,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    start_seed=USER_START_SEED\n",
    ")\n",
    "\n",
    "# 4. Print the result\n",
    "print(\"\\n--- Text Generated from Windowed Model ---\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WL1mPG4eXpZC",
    "outputId": "cdf6019d-f8f0-4fd2-e5dd-12c05b3c4c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../3.model/Markov_chain_v0.3\\markov_model_ws4.json...\n",
      "Model loaded successfully.\n",
      "\n",
      "--- Text Generated from Windowed Model ---\n",
      ". rosebery seems to have disliked his first son , who he claimed looked jewish . on seeing his son for the first time in her career , including cordelia chase from supernatural dramas buffy the vampire slayer and angel , and heather from the syfy horror film voodoo moon 2006 . in an interview , dylan said he had kicked heroin in new york city . they were aboard the booth line steamship ss polycarp . all three men listed their occupations as professors of juitso . after leaving new york , the three men went to the caribbean , where they stayed from september to december 1921 . at some point in the game , and briefly operated his own dealership until forced to close during the automotive industry crisis of 2008 2010 . he has since played a handful of scoreless games , both at the yamaha\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuration\n",
    "WINDOW_SIZE = 4\n",
    "model_dir = \"../3.model/Markov_chain_v0.3\"\n",
    "# We'll make the filename dynamic based on the window size\n",
    "model_filename = f\"markov_model_ws{WINDOW_SIZE}.json\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "# 2. Load or Build the Model\n",
    "if os.path.exists(model_filepath):\n",
    "    windowed_markov_model = load_model_window(model_filepath)\n",
    "else:\n",
    "    print(\"Saved model not found. Training a new one...\")\n",
    "    windowed_markov_model = build_markov_model_with_window(train_data, window_size=WINDOW_SIZE)\n",
    "    save_model_window(windowed_markov_model, model_filepath)\n",
    "\n",
    "# 3. Generate new text from the loaded/trained model\n",
    "USER_START_SEED = \"\"\n",
    "\n",
    "# This line has been updated to pass the start_seed to the function\n",
    "new_text = generate_text_with_window(\n",
    "    windowed_markov_model,\n",
    "    length=150,\n",
    "    window_size=WINDOW_SIZE\n",
    "    #start_seed=USER_START_SEED\n",
    ")\n",
    "\n",
    "# 4. Print the result\n",
    "print(\"\\n--- Text Generated from Windowed Model ---\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t62CRbubcNTj"
   },
   "source": [
    "## Final Optimizations: Grammar and Memory\n",
    "\n",
    "In the final version, we will perform two significant optimizations simultaneously to improve both the quality of the generated text and the model's efficiency.\n",
    "\n",
    "### 1. Teaching the Model Sentence Structure\n",
    "\n",
    "Currently, our model has no sense of the start and end of a sentence. A word at the end of a sentence and the same word in the middle are treated as completely different because of the attached punctuation.\n",
    "\n",
    "> For example, `\"found\"` and `\"found.\"` are two entirely separate words for our model.\n",
    "\n",
    "This is a major flaw, as the model never learns that a period signifies the end of a thought. We solve this by **treating punctuation as its own word (or \"token\")**. During preprocessing, we will pad spaces around major punctuation marks.\n",
    "\n",
    "* **Before:** `\"the cat sat found.\"`\n",
    "* **After:** The text is tokenized into `[\"the\", \"cat\", \"sat\", \"found\", \".\"]`\n",
    "\n",
    "This teaches the model the crucial relationship between words and punctuation, allowing it to learn how to properly end the sentences it generates.\n",
    "\n",
    "### 2. Reducing Memory Footprint with Integer Encoding\n",
    "\n",
    "Storing the entire model using full words (strings) is inefficient. Typically, storing one character requires one byte of memory, so a 7-character word like `\"awesome\"` uses at least 7 bytes. For a large corpus, the memory required for the model dictionary can become enormous.\n",
    "\n",
    "To solve this, we create a simple form of **tokenization** by mapping each unique word in our vocabulary to a unique integer.\n",
    "\n",
    "> For example: `{\"a\": 1, \"the\": 14, \"awesome\": 56, ...}`\n",
    "\n",
    "This fundamentally changes our model. Instead of a single large dictionary, our saved model now consists of **two essential components**:\n",
    "\n",
    "1.  **The Vocabulary Maps:** Two dictionaries, one that maps words to their unique integer IDs (`word_to_int`) and another that maps IDs back to words (`int_to_word`) for generation.\n",
    "2.  **The Core Model:** The main dictionary, which now stores these lightweight integers instead of heavy strings, dramatically reducing its size in memory and on disk.\n",
    "\n",
    "### 3. Decoupling Vocabulary from the Model for Experimentation\n",
    "\n",
    "Since we want to check the performance of our models for different window sizes, it is inefficient to store the vocabulary inside each model file. The vocabulary of the training text is constant; it does not change whether we use a window size of 2, 3, or 4.\n",
    "\n",
    "Therefore, we will adopt a more organized storage strategy:\n",
    "\n",
    "* **Vocabulary (`vocabulary.json`):** The word-to-integer mappings will be built once from the training data and saved to their own separate file.\n",
    "* **Model Dictionaries (`int_model_ws2.json`, etc.):** Each model, trained with a specific window size, will be saved to its own file, named dynamically based on its configuration. These model files will only contain the integer-based transition logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "un0u_aaSZ-sy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and saving new vocabulary...\n",
      "Training final model with window size 3...\n",
      "\n",
      "Generating text...\n",
      "\n",
      "--- Text Generated from Final Model ---\n",
      "the history of the war lent support to the division s infantry brigades. it was well executed. he also pointed out that such knowledge demanded mastery of an artificial reason... his artistic triumph and legendary status were achieved in paris... angry at human stupidity and destructiveness, and within two years of refurbishment and redesign. calling it mcallister tower, 248 units were modernized for residential\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import ast\n",
    "\n",
    "# --- Helper Functions (Placeholders for saving/loading) ---\n",
    "# NOTE: These functions are now very simple because we consistently use string keys.\n",
    "\n",
    "def save_model_window(model, filepath):\n",
    "    \"\"\"Saves the model (with string keys) to a JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(model, f, indent=4)\n",
    "\n",
    "def load_model_window(filepath):\n",
    "    \"\"\"Loads the model (with string keys) from a JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        model = json.load(f)\n",
    "    return model\n",
    "\n",
    "# --- Step 1: Grammar-Aware Preprocessing and Vocabulary Building (Unchanged) ---\n",
    "\n",
    "def tokenize_and_build_vocab(dataset_split):\n",
    "    word_counts = {}\n",
    "    processed_lines = []\n",
    "    for line in dataset_split['text']:\n",
    "        line = re.sub(r'([.,?!])', r' \\1 ', line)\n",
    "        tokens = line.split()\n",
    "        processed_lines.append(tokens)\n",
    "        for token in tokens:\n",
    "            word_counts[token] = word_counts.get(token, 0) + 1\n",
    "    sorted_words = sorted(word_counts.keys(), key=lambda x: word_counts[x], reverse=True)\n",
    "    word_to_int = {word: i for i, word in enumerate(sorted_words)}\n",
    "    int_to_word = {i: word for i, word in enumerate(sorted_words)}\n",
    "    return processed_lines, word_to_int, int_to_word\n",
    "\n",
    "def save_vocab(word_to_int, int_to_word, filepath):\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\"word_to_int\": word_to_int, \"int_to_word\": int_to_word}, f, indent=4)\n",
    "\n",
    "def load_vocab(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    int_to_word_corrected = {int(k): v for k, v in data['int_to_word'].items()}\n",
    "    return data['word_to_int'], int_to_word_corrected\n",
    "\n",
    "# --- Step 2: Build the Integer-Based Markov Model (Corrected Function) ---\n",
    "\n",
    "def build_integer_model(processed_lines, word_to_int, window_size=2):\n",
    "    \"\"\"\n",
    "    Builds a windowed Markov model using STRING keys for consistency.\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    for tokens in processed_lines:\n",
    "        if len(tokens) < window_size + 1:\n",
    "            continue\n",
    "        int_tokens = [word_to_int[token] for token in tokens if token in word_to_int]\n",
    "        \n",
    "        for i in range(len(int_tokens) - window_size):\n",
    "            # Create the tuple state...\n",
    "            current_state_tuple = tuple(int_tokens[i : i + window_size])\n",
    "            # ...and immediately convert it to a string for use as a key.\n",
    "            current_state_str = str(current_state_tuple)\n",
    "            next_word_id = int_tokens[i + window_size]\n",
    "            \n",
    "            # Now, use the string key for all dictionary operations.\n",
    "            if current_state_str not in model:\n",
    "                model[current_state_str] = {}\n",
    "            \n",
    "            next_word_id_str = str(next_word_id)\n",
    "            if next_word_id_str not in model[current_state_str]:\n",
    "                model[current_state_str][next_word_id_str] = 0\n",
    "            model[current_state_str][next_word_id_str] += 1\n",
    "            \n",
    "    return model\n",
    "\n",
    "# --- Step 3: Text Generation with Integer Model (Unchanged) ---\n",
    "\n",
    "def generate_text_integer_model(model, int_to_word, word_to_int, length=75, window_size=2, start_seed=None):\n",
    "    start_state = None\n",
    "    if start_seed:\n",
    "        seed_words = start_seed.lower().split()\n",
    "        if len(seed_words) == window_size and all(word in word_to_int for word in seed_words):\n",
    "            potential_start_state = tuple(word_to_int[word] for word in seed_words)\n",
    "            # Check for the string representation of the tuple in the model keys\n",
    "            if str(potential_start_state) in model:\n",
    "                start_state = potential_start_state\n",
    "\n",
    "    if start_state is None:\n",
    "        # If no valid seed is found, pick a random starting state (which is a string)\n",
    "        random_start_key = random.choice(list(model.keys()))\n",
    "        # Convert the string key back to a tuple for internal processing\n",
    "        start_state = ast.literal_eval(random_start_key)\n",
    "\n",
    "    generated_ids = list(start_state)\n",
    "    current_state_tuple = start_state\n",
    "\n",
    "    for _ in range(length - window_size):\n",
    "        current_state_str = str(current_state_tuple)\n",
    "        if current_state_str not in model:\n",
    "            break\n",
    "        \n",
    "        next_ids_dict = model[current_state_str]\n",
    "        possible_next_ids_str = list(next_ids_dict.keys())\n",
    "        id_frequencies = list(next_ids_dict.values())\n",
    "        \n",
    "        chosen_next_id = random.choices([int(i) for i in possible_next_ids_str], weights=id_frequencies, k=1)[0]\n",
    "        generated_ids.append(chosen_next_id)\n",
    "        current_state_tuple = tuple(generated_ids[-window_size:])\n",
    "\n",
    "    generated_words = [int_to_word.get(id, '?') for id in generated_ids]\n",
    "    return ' '.join(generated_words).replace(' .', '.').replace(' ,', ',').replace(' ?', '?').replace(' !', '!')\n",
    "\n",
    "# --- Main Execution (Simplified Logic) ---\n",
    "\n",
    "# ASSUMPTION: You have a Hugging Face dataset object named `train_data` loaded.\n",
    "# For example:\n",
    "# from datasets import load_dataset\n",
    "# wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "# train_data = wikitext['train']\n",
    "\n",
    "MODEL_DIR = \"../3.model/Markov_chain_v0.4\"\n",
    "VOCAB_FILEPATH = os.path.join(MODEL_DIR, \"vocabulary4Markov_chain_v0.4.json\")\n",
    "WINDOW_SIZE = 3 \n",
    "MODEL_FILENAME = f\"int_model_ws{WINDOW_SIZE}.json\"\n",
    "MODEL_FILEPATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "# 1. Build or Load Vocabulary\n",
    "if os.path.exists(VOCAB_FILEPATH):\n",
    "    print(\"Loading existing vocabulary...\")\n",
    "    word_to_int, int_to_word = load_vocab(VOCAB_FILEPATH)\n",
    "    processed_lines, _, _ = tokenize_and_build_vocab(train_data)\n",
    "else:\n",
    "    print(\"Building and saving new vocabulary...\")\n",
    "    processed_lines, word_to_int, int_to_word = tokenize_and_build_vocab(train_data)\n",
    "    save_vocab(word_to_int, int_to_word, VOCAB_FILEPATH)\n",
    "\n",
    "# 2. Build or Load the Integer Model\n",
    "if os.path.exists(MODEL_FILEPATH):\n",
    "    print(f\"Loading final model with window size {WINDOW_SIZE}...\")\n",
    "    final_model = load_model_window(MODEL_FILEPATH)\n",
    "else:\n",
    "    print(f\"Training final model with window size {WINDOW_SIZE}...\")\n",
    "    # This now directly returns the model with the correct string keys ✅\n",
    "    final_model = build_integer_model(processed_lines, word_to_int, window_size=WINDOW_SIZE)\n",
    "    # Save the model (which already has string keys, perfect for JSON)\n",
    "    save_model_window(final_model, MODEL_FILEPATH)\n",
    "    # The confusing conversion line is no longer needed.\n",
    "\n",
    "# 3. Generate Text\n",
    "print(\"\\nGenerating text...\")\n",
    "generated_text = generate_text_integer_model(\n",
    "    final_model,\n",
    "    int_to_word,\n",
    "    word_to_int,\n",
    "    length=75,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    start_seed=\"the history of\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Text Generated from Final Model ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Conceptual Limit and the Path Forward\n",
    "\n",
    "After implementing a series of powerful optimizations—memory-efficient processing, increased context windows, grammar-aware tokenization, and integer encoding—we have pushed the Markov chain architecture to its logical peak. This is the point where we must acknowledge the fundamental limitations of the model itself.\n",
    "\n",
    "### The Inescapable Roadblock: Statistics vs. Semantics\n",
    "\n",
    "Our model has become an incredibly sophisticated pattern-matching engine. It knows which words are statistically likely to follow other sequences of words. However, it has no ability to grasp the actual **meaning**, or **semantics**, of the words it is processing.\n",
    "\n",
    "To the model, the words \"king,\" \"queen,\" and \"cabbage\" are just unique integers. It does not understand that two of those words relate to royalty and one relates to vegetables. It only knows the statistical probabilities of their arrangement. This is the core conceptual roadblock of the Markov assumption: **it can mimic structure, but it cannot comprehend meaning**.\n",
    "\n",
    "### Final Refinements vs. New Architectures\n",
    "\n",
    "This is where most of the *architectural* optimizations for a Markov chain end. While we have reached the limit of what this type of model can conceptually do, there are still two key refinements we could make. These wouldn't change the fundamental nature of the model, but they would improve its performance and robustness.\n",
    "\n",
    "#### 1. Performance Optimization: Sparse Matrices\n",
    "\n",
    "* **What:** Instead of a nested Python dictionary, we could represent our model as a **sparse matrix** using libraries like `scipy.sparse`. The rows would represent the state (the tuple of previous words), the columns would represent the next possible word, and the cell value would be the transition count or probability.\n",
    "* **Why:**\n",
    "    * **Speed:** Mathematical operations on these matrices (like calculating probabilities) are performed using highly optimized C code (a process called vectorization), which is orders of magnitude faster than looping through a Python dictionary.\n",
    "    * **Memory:** While our integer-encoded dictionary is very good, a sparse matrix has even less memory overhead per entry, making it the superior choice for extremely large vocabularies.\n",
    "\n",
    "#### 2. Quality Optimization: Smoothing and Backoff\n",
    "\n",
    "* **What:** What happens if the model generates a sequence of words it has never seen before in the training data? It will have no entry for this state and will crash or stop generating text. **Smoothing** (like Laplace smoothing) is a technique that gives a tiny, non-zero probability to every *possible* transition, even unseen ones. A more advanced technique, **Backoff**, would make the model \"back off\" to a smaller context window (e.g., if it can't find a match for a 3-word sequence, it tries matching on the last 2 words) to find a next step.\n",
    "* **Why:** This makes the model more **robust**. It prevents it from getting stuck on unfamiliar phrases and allows it to generate longer, more fluent sequences without failing, which directly improves the quality and reliability of the output.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While these refinements would make our model faster and more resilient, they do not solve the core problem of semantic understanding. To achieve a more human-like grasp of language—one that understands context, topics, and long-range dependencies—we would need to move beyond Markov chains to a different class of models entirely: **neural networks**, such as Recurrent Neural Networks (LSTMs) or the state-of-the-art Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
