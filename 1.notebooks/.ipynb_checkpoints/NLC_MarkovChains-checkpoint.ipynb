{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Our Development Philosophy: From Baseline to Optimized\n",
        "\n",
        "This project follows a deliberate and iterative approach to model development. Our core principle is to ensure that every change is measured, understood, and contributes positively to the final result.\n",
        "\n",
        "### Phase 1: Build the Brute-Force Baseline\n",
        "\n",
        "First, we create the simplest possible, end-to-end working model. This **bare-minimum** version serves two critical purposes:\n",
        "\n",
        "1.  **Proof of Concept:** It confirms that our data pipeline and basic architecture are functional.\n",
        "2.  **Establish a Benchmark:** It provides a clear **baseline performance metric**. All future work will be measured against this initial score.\n",
        "\n",
        "### Phase 2: Optimize One Step at a Time\n",
        "\n",
        "Once the baseline is established, we begin a cycle of incremental improvement. We strictly adhere to the principle of making **one isolated change at a time**.\n",
        "\n",
        "Instead of overhauling the model at once, we will:\n",
        "\n",
        "* **Target** a single component for improvement (e.g., hyperparameter tuning, feature engineering, architecture modification).\n",
        "* **Implement** that one specific change.\n",
        "* **Measure** its exact impact on performance.\n",
        "\n",
        "This methodical process removes guesswork and allows us to attribute performance gains or losses directly to a specific action."
      ],
      "metadata": {
        "id": "SQbkSYL8DhPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "my_custom_cache_dir = \"/content/data/default_data\"\n",
        "wikitext_dataset = load_dataset(\n",
        "    \"wikitext\",\n",
        "    \"wikitext-2-raw-v1\",\n",
        "    cache_dir=my_custom_cache_dir\n",
        ")\n",
        "print(wikitext_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSS3p4etDy-C",
        "outputId": "8da2a36b-12c7-4301-a702-efcf9010f430"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 4358\n",
            "    })\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 36718\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3760\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Exploration\n",
        "test=wikitext_dataset['test']\n",
        "train=wikitext_dataset['train']\n",
        "validation=wikitext_dataset['validation']\n",
        "\n",
        "for i in range(0,25):\n",
        "  print(f\"{i}:{train[i]['text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7eMZxqOFVyA",
        "outputId": "0ec87883-ac4a-4b1e-9fff-fbe96156146f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\n",
            "1: = Valkyria Chronicles III = \n",
            "\n",
            "2:\n",
            "3: Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
            "\n",
            "4: The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
            "\n",
            "5: It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n",
            "\n",
            "6:\n",
            "7: = = Gameplay = = \n",
            "\n",
            "8:\n",
            "9: As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \n",
            "\n",
            "10: The game 's battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \n",
            "\n",
            "11: Troops are divided into five classes : Scouts , Shocktroopers , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types . \n",
            "\n",
            "12:\n",
            "13: = = Plot = = \n",
            "\n",
            "14:\n",
            "15: The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , Altaha Abilia , meaning \" Always Ready . \" The three main characters are No.7 Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace No.1 Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and No.13 Riela Marcellis , a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers . \n",
            "\n",
            "16: As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , Gusurg , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \n",
            "\n",
            "17: Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient Valkyrian super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last trump card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the Valkyrian weapon . Each member then goes their separate ways in order to begin their lives anew . \n",
            "\n",
            "18:\n",
            "19: = = Development = = \n",
            "\n",
            "20:\n",
            "21: Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development beginning shortly after this . The director of Valkyria Chronicles II , Takeshi Ozawa , returned to that role for Valkyria Chronicles III . Development work took approximately one year . After the release of Valkyria Chronicles II , the staff took a look at both the popular response for the game and what they wanted to do next for the series . Like its predecessor , Valkyria Chronicles III was developed for PlayStation Portable : this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II , and they had not come up with the \" revolutionary \" idea that would warrant a new entry for the PlayStation 3 . Speaking in an interview , it was stated that the development team considered Valkyria Chronicles III to be the series ' first true sequel : while Valkyria Chronicles II had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of Valkyria Chronicles II due to being on the same platform . In addition to Sega staff from the previous games , development work was also handled by Media.Vision. The original scenario was written Kazuki Yamanobe , while the script was written by Hiroyuki Fujii , Koichi Majima , Kishiko Miyagi , Seiki Nagakawa and Takayuki Shouji . Its story was darker and more somber than that of its predecessor . \n",
            "\n",
            "22: The majority of material created for previous games , such as the BLiTZ system and the design of maps , was carried over . Alongside this , improvements were made to the game 's graphics and some elements were expanded , such as map layouts , mission structure , and the number of playable units per mission . A part of this upgrade involved creating unique polygon models for each character 's body . In order to achieve this , the cooperative elements incorporated into the second game were removed , as they took up a large portion of memory space needed for the improvements . They also adjusted the difficulty settings and ease of play so they could appeal to new players while retaining the essential components of the series ' gameplay . The newer systems were decided upon early in development . The character designs were done by Raita Honjou , who had worked on the previous Valkyria Chronicles games . When creating the Nameless Squad , Honjou was faced with the same problem he had had during the first game : the military uniforms essentially destroyed character individuality , despite him needing to create unique characters the player could identify while maintaining a sense of reality within the Valkyria Chronicles world . The main color of the Nameless was black . As with the previous Valkyria games , Valkyria Chronicles III used the CANVAS graphics engine . The anime opening was produced by Production I.G. \n",
            "\n",
            "23:\n",
            "24: = = = Music = = = \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirement Observed:\n",
        "\n",
        "\n",
        "*   Remove non english characters\n",
        "*   Remove lines starting with = = =\n",
        "*   Remove @-@\n",
        "*   Casual Preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "tKOkr78xHxzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before Preprocessing:\n",
        "# Calculate the total number of characters for each split\n",
        "train_chars = sum(len(line) for line in train['text'] if line)\n",
        "validation_chars = sum(len(line) for line in validation['text'] if line)\n",
        "test_chars = sum(len(line) for line in test['text'] if line)\n",
        "\n",
        "print(f\"Training Data:   {train.num_rows:,} rows, {train_chars:,} characters\")\n",
        "print(f\"Validation Data: {validation.num_rows:,} rows, {validation_chars:,} characters\")\n",
        "print(f\"Test Data:       {test.num_rows:,} rows, {test_chars:,} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DydskqYTFgsF",
        "outputId": "f430b032-f7e3-4782-b3fd-bf53c76eaf08"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:   36,718 rows, 10,892,990 characters\n",
            "Validation Data: 3,760 rows, 1,142,150 characters\n",
            "Test Data:       4,358 rows, 1,285,622 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# --- Before Cleaning ---\n",
        "print(\"--- Row Counts Before Cleaning ---\")\n",
        "print(f\"Train:      {wikitext_dataset['train'].num_rows:,}\")\n",
        "print(f\"Validation: {wikitext_dataset['validation'].num_rows:,}\")\n",
        "print(f\"Test:       {wikitext_dataset['test'].num_rows:,}\")\n",
        "\n",
        "\n",
        "# --- Preprocessing Steps ---\n",
        "# These operations are applied to all splits (train, validation, test) simultaneously.\n",
        "processed_dataset = wikitext_dataset.filter(\n",
        "    lambda example: not example['text'].strip().startswith(' = ')\n",
        ")\n",
        "\n",
        "processed_dataset = processed_dataset.map(\n",
        "    lambda example: {\n",
        "        'text': re.sub(\n",
        "            r'[^a-zA-Z0-9\\s.,\\'?!-]', '',\n",
        "            example['text'].lower().replace('@-@', '')\n",
        "        ).strip()\n",
        "    }\n",
        ")\n",
        "\n",
        "processed_dataset = processed_dataset.filter(\n",
        "    lambda example: len(example['text']) > 0\n",
        ")\n",
        "\n",
        "\n",
        "# --- After Cleaning ---\n",
        "print(\"\\n--- Row Counts After Cleaning ---\")\n",
        "print(f\"Train:      {processed_dataset['train'].num_rows:,}\")\n",
        "print(f\"Validation: {processed_dataset['validation'].num_rows:,}\")\n",
        "print(f\"Test:       {processed_dataset['test'].num_rows:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li5oyf7PGZ47",
        "outputId": "fd6be37e-0789-496a-d35d-96bd6135b833"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Row Counts Before Cleaning ---\n",
            "Train:      36,718\n",
            "Validation: 3,760\n",
            "Test:       4,358\n",
            "\n",
            "--- Row Counts After Cleaning ---\n",
            "Train:      23,764\n",
            "Validation: 2,461\n",
            "Test:       2,891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Exploration\n",
        "test=processed_dataset['test']\n",
        "train=processed_dataset['train']\n",
        "validation=processed_dataset['validation']\n",
        "\n",
        "for i in range(26):\n",
        "  print(f\"{i}:{train[i]['text']}\")\n",
        "\n",
        "  #no immediate issues found"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCH8m9xVK52I",
        "outputId": "04824f3d-eeb7-4240-ab41-1aba1c299fa8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:valkyria chronicles iii\n",
            "1:senj no valkyria 3  unrecorded chronicles  japanese  3 , lit . valkyria of the battlefield 3  , commonly referred to as valkyria chronicles iii outside japan , is a tactical role  playing video game developed by sega and media.vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . employing the same fusion of tactical and real  time gameplay as its predecessors , the story runs parallel to the first game and follows the  nameless  , a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit  calamaty raven  .\n",
            "2:the game began development in 2010 , carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . character designer raita honjou and composer hitoshi sakimoto both returned from previous entries , along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game 's opening theme was sung by may 'n .\n",
            "3:it met with positive sales in japan , and was praised by both japanese and western critics . after release , it received downloadable content , along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . media.vision would return to the franchise with the development of valkyria  azure revolution for the playstation 4 .\n",
            "4:gameplay\n",
            "5:as with previous valkyira chronicles games , valkyria chronicles iii is a tactical role  playing game where players take control of a military unit and take part in missions against enemy forces . stories are told through comic book  like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . the player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . the route to each story location on the map varies depending on an individual player 's approach  when one option is selected , the other is sealed off to the player . outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . alongside the main story missions are character  specific sub missions relating to different squad members . after the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . there are also love simulation elements related to the game 's two main heroines , although they take a very minor role .\n",
            "6:the game 's battle system , the blitz system , is carried over directly from valkyira chronicles . during missions , players select each unit using a top  down perspective of the battlefield map  once a character is selected , the player moves the character around the battlefield in third  person . a character can only act once per  turn , but characters can be granted multiple turns at the expense of other characters ' turns . each character has a field and distance of movement limited by their action gauge . up to nine characters can be assigned to a single mission . during gameplay , characters will call out if something happens to them , such as their health points  hp  getting low or being knocked out by enemy attacks . each character has specific  potentials  , skills unique to each character . they are divided into  personal potential  , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and  battle potentials  , which are grown throughout the game and always grant boons to a character . to learn battle potentials , each character has a unique  masters table  , a grid  based skill table that can be used to acquire and link different skills . characters also have special abilities that grant them temporary boosts on the battlefield  kurt can activate  direct command  and move around the battlefield without depleting his action point gauge , the character reila can shift into her  valkyria form  and become invincible , while imca can target multiple enemy units with her heavy weapon .\n",
            "7:troops are divided into five classes  scouts , shocktroopers , engineers , lancers and armored soldier . troopers can switch classes by changing their assigned weapon . changing class does not greatly affect the stats gained while in a previous class . with victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types .\n",
            "8:plot\n",
            "9:the game takes place during the second europan war . gallian army squad 422 , also known as  the nameless  , are a penal military unit composed of criminals , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . ordered by the gallian military to perform the most dangerous missions that the regular army and militia will not do , they are nevertheless up to the task , exemplified by their motto , altaha abilia , meaning  always ready .  the three main characters are no.7 kurt irving , an army officer falsely accused of treason who wishes to redeem himself  ace no.1 imca , a female darcsen heavy weapons specialist who seeks revenge against the valkyria who destroyed her home  and no.13 riela marcellis , a seemingly jinxed young woman who is unknowingly a descendant of the valkyria . together with their fellow squad members , these three are tasked to fight against a mysterious imperial unit known as calamity raven , consisting of mostly darcsen soldiers .\n",
            "10:as the nameless officially do not exist , the upper echelons of the gallian army exploit the concept of plausible deniability in order to send them on missions that would otherwise make gallia lose face in the war . while at times this works to their advantage , such as a successful incursion into imperial territory , other orders cause certain members of the 422nd great distress . one such member , gusurg , becomes so enraged that he abandons his post and defects into the ranks of calamity raven , attached to the ideal of darcsen independence proposed by their leader , dahau . at the same time , elements within gallian army command move to erase the nameless in order to protect their own interests . hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the gallian war effort . this continues until the nameless 's commanding officer , ramsey crowe , who had been kept under house arrest , is escorted to the capital city of randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the gallian general that had accused kurt of treason .\n",
            "11:partly due to these events , and partly due to the major losses in manpower gallia suffers towards the end of the war with the empire , the nameless are offered a formal position as a squad in the gallian army rather than serve as an anonymous shadow force . this is short  lived , however , as following maximilian 's defeat , dahau and calamity raven move to activate an ancient valkyrian super weapon within the empire , kept secret by their benefactor . without the support of maximilian or the chance to prove themselves in the war with gallia , it is dahau 's last trump card in creating a new darcsen nation . as an armed gallian force invading the empire just following the two nations ' cease  fire would certainly wreck their newfound peace , kurt decides to once again make his squad the nameless , asking crowe to list himself and all under his command as killed  in  action . now owing allegiance to none other than themselves , the 422nd confronts dahau and destroys the valkyrian weapon . each member then goes their separate ways in order to begin their lives anew .\n",
            "12:development\n",
            "13:concept work for valkyria chronicles iii began after development finished on valkyria chronicles ii in early 2010 , with full development beginning shortly after this . the director of valkyria chronicles ii , takeshi ozawa , returned to that role for valkyria chronicles iii . development work took approximately one year . after the release of valkyria chronicles ii , the staff took a look at both the popular response for the game and what they wanted to do next for the series . like its predecessor , valkyria chronicles iii was developed for playstation portable  this was due to the team wanting to refine the mechanics created for valkyria chronicles ii , and they had not come up with the  revolutionary  idea that would warrant a new entry for the playstation 3 . speaking in an interview , it was stated that the development team considered valkyria chronicles iii to be the series ' first true sequel  while valkyria chronicles ii had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of valkyria chronicles ii due to being on the same platform . in addition to sega staff from the previous games , development work was also handled by media.vision. the original scenario was written kazuki yamanobe , while the script was written by hiroyuki fujii , koichi majima , kishiko miyagi , seiki nagakawa and takayuki shouji . its story was darker and more somber than that of its predecessor .\n",
            "14:the majority of material created for previous games , such as the blitz system and the design of maps , was carried over . alongside this , improvements were made to the game 's graphics and some elements were expanded , such as map layouts , mission structure , and the number of playable units per mission . a part of this upgrade involved creating unique polygon models for each character 's body . in order to achieve this , the cooperative elements incorporated into the second game were removed , as they took up a large portion of memory space needed for the improvements . they also adjusted the difficulty settings and ease of play so they could appeal to new players while retaining the essential components of the series ' gameplay . the newer systems were decided upon early in development . the character designs were done by raita honjou , who had worked on the previous valkyria chronicles games . when creating the nameless squad , honjou was faced with the same problem he had had during the first game  the military uniforms essentially destroyed character individuality , despite him needing to create unique characters the player could identify while maintaining a sense of reality within the valkyria chronicles world . the main color of the nameless was black . as with the previous valkyria games , valkyria chronicles iii used the canvas graphics engine . the anime opening was produced by production i.g.\n",
            "15:music\n",
            "16:the music was composed by hitoshi sakimoto , who had also worked on the previous valkyria chronicles games . when he originally heard about the project , he thought it would be a light tone similar to other valkyria chronicles games , but found the themes much darker than expected . an early theme he designed around his original vision of the project was rejected . he redid the main theme about seven times through the music production due to this need to reassess the game . the main theme was initially recorded using orchestra , then sakimoto removed elements such as the guitar and bass , then adjusted the theme using a synthesizer before redoing segments such as the guitar piece on their own before incorporating them into the theme . the rejected main theme was used as a hopeful tune that played during the game 's ending . the battle themes were designed around the concept of a  modern battle  divorced from a fantasy scenario by using modern musical instruments , constructed to create a sense of atonality . while sakimoto was most used to working with synthesized music , he felt that he needed to incorporate live instruments such as orchestra and guitar . the guitar was played by mitsuhiro ohta , who also arranged several of the later tracks . the game 's opening theme song ,  if you wish for ...    , moshimo kimi ga negauno nara  , was sung by japanese singer may 'n . its theme was the reason soldiers fought , in particular their wish to protect what was precious to them rather than a sense of responsibility or duty . its lyrics were written by seiko fujibayashi , who had worked on may 'n on previous singles .\n",
            "17:release\n",
            "18:in september 2010 , a teaser website was revealed by sega , hinting at a new valkyria chronicles game . in its september issue , famitsu listed that senj no valkyria 3 would be arriving on the playstation portable . its first public appearance was at the 2010 tokyo game show  tgs  , where a demo was made available for journalists and attendees . during the publicity , story details were kept scant so as not to spoil too much for potential players , along with some of its content still being in flux at the time of its reveal . to promote the game and detail the story leading into the game 's events , an episodic flash visual novel written by fujii began release in january 2011 . the game was released january 27 , 2011 . during an interview , the development team said that the game had the capacity for downloadable content  dlc  , but that no plans were finalized . multiple dlc maps , featuring additional missions and recruitable characters , were released between february and april 2011 . an expanded edition of the game , valkyria chronicles iii extra edition , released on november 23 , 2011 . packaged and sold at a lower price than the original , extra edition game with seven additional episodes  three new , three chosen by staff from the game 's dlc , and one made available as a pre  order bonus . people who also owned the original game could transfer their save data between versions .\n",
            "19:unlike its two predecessors , valkyria chronicles iii was not released in the west . according to sega , this was due to poor sales of valkyria chronicles ii and the general unpopularity of the psp in the west . an unofficial fan translation patch began development in february 2012  players with a copy of valkyria chronicles iii could download and apply the patch , which translated the game 's text into english . compatible with the extra edition , the patch was released in january 2014 .\n",
            "20:reception\n",
            "21:on its day of release in japan , valkyria chronicles iii topped both platform  exclusive and multi  platform sales charts . by early february , the game sold 102 , 779 units , coming in second overall to the last story for the wii . by the end of the year , the game had sold just over 152 , 500 units .\n",
            "22:famitsu enjoyed the story , and were particularly pleased with the improvements to gameplay . japanese gaming site game watch impress , despite negatively noting its pacing and elements recycled from previous games , was generally positive about its story and characters , and found its gameplay entertaining despite off  putting difficulty spikes . 4gamer.net writer naohiko misuosame , in a  play test  article based on the game 's psn demo , felt that valkyria chronicles iii provided a  profound feeling of closure  for the valkyria chronicles series . he praised its gameplay despite annoying limitations to aspects such as special abilities , and positively noted its shift in story to a tone similar to the first game .\n",
            "23:playstation official magazine - uk praised the story 's blurring of gallia 's moral standing , art style , and most points about its gameplay , positively noting the latter for both its continued quality and the tweaks to balance and content . its one major criticism were multiple difficulty spikes , something that had affected the previous games . heath hindman of gaming website playstation lifestyle praised the addition of non  linear elements and improvements or removal of mechanics from valkyria chronicles ii in addition to praising the returning gameplay style of previous games . he also positively noted the story 's serious tone . points criticized in the review were recycled elements , awkward cutscenes that seemed to include all characters in a scene for no good reason , pacing issues , and occasional problems with the game 's ai .\n",
            "24:in a preview of the tgs demo , ryan geddes of ign was left excited as to where the game would go after completing the demo , along with enjoying the improved visuals over valkyria chronicles ii . kotaku 's richard eisenbeis was highly positive about the game , citing is story as a return to form after valkyria chronicles ii and its gameplay being the best in the series . his main criticisms were its length and gameplay repetition , along with expressing regret that it would not be localized .\n",
            "25:legacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Markov Chain Model for Text Generation\n",
        "\n",
        "A Markov chain is a simple probabilistic model used to predict the next event in a sequence based solely on the current event. When applied to text, it predicts the next word based only on the current word, ignoring all previous context.\n",
        "\n",
        "### What is \"The Model\"?\n",
        "\n",
        "In the context of this project, the \"model\" is not a complex algorithm but a simple and intuitive data structure: a **Python dictionary**. This dictionary is the final output of the \"training\" process. It stores all the learned word-to-word transition probabilities from the input text.\n",
        "\n",
        "### The Structure of the Model (The Dictionary)\n",
        "\n",
        "The model is a nested dictionary with a specific structure: `word -> {next_word: count}`.\n",
        "\n",
        "* **Level 1: The Keys (Current Words)**\n",
        "    * The keys of the main dictionary are all the unique words found in the text. Each key represents a possible \"current state.\"\n",
        "\n",
        "* **Level 2: The Values (Next Words and Their Frequencies)**\n",
        "    * The value associated with each key is *another dictionary*.\n",
        "    * In this inner dictionary, the keys are all the words that have ever appeared immediately after the \"current word.\"\n",
        "    * The values are the counts (integers) of how many times that specific transition occurred.\n",
        "\n",
        "### A Concrete Example\n",
        "\n",
        "If our training text is: \"**the cat sat on the mat**\"\n",
        "\n",
        "The resulting model dictionary would look like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"the\": {\n",
        "        \"cat\": 1,\n",
        "        \"mat\": 1\n",
        "    },\n",
        "    \"cat\": {\n",
        "        \"sat\": 1\n",
        "    },\n",
        "    \"sat\": {\n",
        "        \"on\": 1\n",
        "    },\n",
        "    \"on\": {\n",
        "        \"the\": 1\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "PIJMdwzPN5_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import json\n",
        "\n",
        "\n",
        "def build_markov_model(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    Builds a simple Markov chain model from a given text.\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "\n",
        "    model = {}\n",
        "\n",
        "    for i in range(len(tokens) - 1):\n",
        "        current_word = tokens[i]\n",
        "        next_word = tokens[i + 1]\n",
        "\n",
        "        if current_word not in model:\n",
        "            model[current_word] = {}\n",
        "\n",
        "        if next_word not in model[current_word]:\n",
        "            model[current_word][next_word] = 0\n",
        "\n",
        "        model[current_word][next_word] += 1\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_text(model: dict, length: int = 50,start: str=None) -> str:\n",
        "    if start is None:\n",
        "      start_word=random.choice(list(model.keys()))\n",
        "    else:\n",
        "        start_word=start.lower()\n",
        "    \"\"\"\n",
        "    Generates new text using a pre-built Markov model.\n",
        "    \"\"\"\n",
        "    generated_text = [start_word]\n",
        "    current_word = start_word\n",
        "\n",
        "    for _ in range(length - 1):\n",
        "        if current_word not in model:\n",
        "            break\n",
        "\n",
        "        next_words_dict = model[current_word]\n",
        "        possible_next_words = list(next_words_dict.keys())\n",
        "        word_frequencies = list(next_words_dict.values())\n",
        "\n",
        "        chosen_next_word = random.choices(possible_next_words, weights=word_frequencies, k=1)[0]\n",
        "\n",
        "        generated_text.append(chosen_next_word)\n",
        "        current_word = chosen_next_word\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "\n",
        "# Helper Functions Ignore them\n",
        "import os\n",
        "model_filepath = \"/content/model/Markov_chain_v0.1\" # <-- Change this path\n",
        "\n",
        "def save_model(model, filepath):\n",
        "    \"\"\"Saves the model dictionary to a JSON file.\"\"\"\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        json.dump(model, f)\n",
        "\n",
        "def load_model(filepath):\n",
        "    \"\"\"Loads the model dictionary from a JSON file.\"\"\"\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# --- Main execution ---\n",
        "\n",
        "# 1. Convert the 'train' dataset object into a single corpus string\n",
        "# We join all the lines of text together with spaces in between.\n",
        "print(\"Preparing training corpus...\")\n",
        "corpus = \" \".join(train['text'])\n",
        "print(\"Corpus prepared.\")\n",
        "\n",
        "# 2. Build the model using the training corpus\n",
        "print(\"Building Markov model...\")\n",
        "if os.path.exists(model_filepath):\n",
        "    print(\"Loading saved model...\")\n",
        "    markov_model = load_model(model_filepath)\n",
        "else:\n",
        "    print(\"No saved model found. Building a new one...\")\n",
        "    markov_model = build_markov_model(corpus)\n",
        "    save_model(markov_model, model_filepath)\n",
        "\n",
        "\n",
        "# 3. Generate new text from the trained model\n",
        "new_text = generate_text(markov_model, length=75,start=\"morning\")\n",
        "\n",
        "# 4. Print the result\n",
        "print(\"\\n--- Text Generated from WikiText Model ---\")\n",
        "print(new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDXONapyNFhX",
        "outputId": "82e77f97-aa74-4760-bc30-befeb8e9b3c9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training corpus...\n",
            "Corpus prepared.\n",
            "Building Markov model...\n",
            "Loading saved model...\n",
            "\n",
            "--- Text Generated from WikiText Model ---\n",
            "morning sun , the needs food to be echoed the overcrowding and for the death . according to 10 , whom were false bony fishes , and brown black people simply left colonel henry prevented the restaurant 17 side west . f4 ng6 11.d4 d5 ! 2012 13 . route 61 sonatine , which was also published book containing psychoactive . the song was installed later that i hadn t corporation , which attracts an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizations\n",
        "\n",
        "### Memory Efficiency: Incremental Processing\n",
        "\n",
        "The initial brute-force approach required loading the entire training dataset into a **single, massive string**. This is highly problematic for large datasets for two main reasons:\n",
        "\n",
        "1.  **High Memory Usage:** It can consume several gigabytes of RAM, potentially crashing the program with a `MemoryError` on systems with limited memory.\n",
        "2.  **Loss of Context:** It destroys natural sentence boundaries, leading to a lower-quality model that can't learn to start or end sentences properly.\n",
        "\n",
        "To solve this, we **fragment the training process**. Instead of creating one large object in memory, the optimized approach is to build the model **incrementally**.\n",
        "\n",
        "We iterate through the dataset **line-by-line**, processing only one line at a time to update our model dictionary. This is a highly **memory-efficient** solution, as it allows us to train on a dataset of virtually any size using **little to no additional space**."
      ],
      "metadata": {
        "id": "U5fri8tWSehW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_markov_model_incrementally(dataset_split):\n",
        "    \"\"\"\n",
        "    Builds a model by iterating through the dataset line by line (memory-efficient).\n",
        "    \"\"\"\n",
        "    model = {}\n",
        "    for line in dataset_split['text']:\n",
        "        tokens = line.split()\n",
        "        if not tokens:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(tokens) - 1):\n",
        "            current_word = tokens[i]\n",
        "            next_word = tokens[i + 1]\n",
        "\n",
        "            if current_word not in model:\n",
        "                model[current_word] = {}\n",
        "\n",
        "            if next_word not in model[current_word]:\n",
        "                model[current_word][next_word] = 0\n",
        "\n",
        "            model[current_word][next_word] += 1\n",
        "\n",
        "    return model\n",
        "# --- Main Execution (Incremental Approach) ---\n",
        "# Assumes 'train' dataset object and all helper functions are pre-defined.\n",
        "\n",
        "# 1. Configuration\n",
        "MODEL_FILEPATH = \"/content/model/Markov_chain_v0.2\"\n",
        "START_WORD = \"morning\"\n",
        "TEXT_LENGTH = 75\n",
        "\n",
        "# 2. Load or Build the Model Incrementally\n",
        "if os.path.exists(MODEL_FILEPATH):\n",
        "    print(\"Loading existing model...\")\n",
        "    markov_model = load_model(MODEL_FILEPATH)\n",
        "else:\n",
        "    print(\"No existing model found. Building a new one incrementally...\")\n",
        "\n",
        "    # This is the corrected, memory-efficient call.\n",
        "    # It passes the dataset object directly to the incremental builder.\n",
        "    markov_model = build_markov_model_incrementally(train)\n",
        "\n",
        "    save_model(markov_model, MODEL_FILEPATH)\n",
        "\n",
        "print(\"Model is ready.\")\n",
        "\n",
        "# 3. Generate and Print Text\n",
        "print(\"\\n--- Generating new text ---\")\n",
        "generated_text = generate_text(\n",
        "    markov_model,\n",
        "    length=TEXT_LENGTH,\n",
        "    start=START_WORD\n",
        ")\n",
        "\n",
        "print(\"\\n--- Generated Text ---\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcuTb1jaOoc3",
        "outputId": "12cf38a6-9990-4f90-a9f3-71d66816e3fa"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No existing model found. Building a new one incrementally...\n",
            "Model is ready.\n",
            "\n",
            "--- Generating new text ---\n",
            "\n",
            "--- Generated Text ---\n",
            "morning , along with several organizations began in august 27 more than a moment i never having open runners , eno about 70 lines would have been to the fox banksia , who is known and underground organizations of x. british flag of all unnatural proportions of keamy shoots it back onto the second division undergraduate he visits to distance events are deeply flawed attempt to catch them as a new stage career . in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Improvements\n",
        "\n",
        "### Increasing Context with a Larger Window Size\n",
        "\n",
        "The most significant weakness of the brute-force model is its extremely limited memory. It is a **first-order Markov chain**, meaning it only knows the previous **1** word.\n",
        "\n",
        "This leads to a critical failure in logical consistency. For example, consider the phrase:\n",
        "\n",
        "> \"The sky is...\"\n",
        "\n",
        "You are correct that the model is still probabilistic. If its training resulted in a model entry like:\n",
        "```json\n",
        "{ \"is\": {\"apple\": 3, \"blue\": 4} }\n",
        "```\n",
        "## The Problem of Lost Context\n",
        "\n",
        "The model correctly knows that \"**blue**\" is a more likely successor to \"**is**\" than \"**apple**.\"\n",
        "\n",
        "The core failure is the **loss of critical context**. The model's pool of possible next words is created from *every single instance* where the word \"is\" appeared, regardless of what came before it. It learns from unrelated phrases like:\n",
        "\n",
        "> * \"the sky **is** blue\"\n",
        "> * \"the company's logo **is** an apple\"\n",
        "\n",
        "When the model's only context is the word \"is,\" it blends these unrelated scenarios. It correctly identifies \"blue\" as more probable, but \"apple\" remains a possibility because the model has completely forgotten the crucial preceding word, \"**sky**.\"\n",
        "\n",
        "## The Solution: Using Tuples for State\n",
        "\n",
        "To solve this, we use a **tuple of multiple words** as the key. This acts as our new \"state,\" effectively increasing the model's memory and filtering out irrelevant choices.\n",
        "\n",
        "* **Old Model State (Plain Word):** `is`\n",
        "    > Possible Next Words:\n",
        "    > ```json\n",
        "    > { \"apple\": 3, \"blue\": 4, \"green\": 2, ... }\n",
        "    > ```\n",
        "\n",
        "* **New Model State (Tuple):** `('sky', 'is')`\n",
        "    > Possible Next Words:\n",
        "    > ```json\n",
        "    > { \"blue\": 150, \"clear\": 80, \"overcast\": 45, ... }\n",
        "    > ```\n",
        "\n",
        "By using a tuple, the model's state is now `('sky', 'is')`. The pool of possible next words it looks at is now completely different and far more relevant. The word \"apple\" is unlikely to even be in this new list, leading to much more coherent and context-aware text generation."
      ],
      "metadata": {
        "id": "U9Ng2W3AUw3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import json\n",
        "import os\n",
        "import ast # Used to safely convert string representations of tuples back to tuples\n",
        "\n",
        "# (Assuming 'processed_dataset' is already created and split into train, validation, test)\n",
        "# train = processed_dataset['train']\n",
        "\n",
        "def build_markov_model_with_window(dataset_split, window_size=2):\n",
        "    \"\"\"\n",
        "    Builds a higher-order Markov model using a tuple of words (the \"window\") as the state.\n",
        "    \"\"\"\n",
        "    model = {}\n",
        "    for line in dataset_split['text']:\n",
        "        tokens = line.split()\n",
        "        if len(tokens) < window_size + 1:\n",
        "            continue\n",
        "        for i in range(len(tokens) - window_size):\n",
        "            current_state = tuple(tokens[i : i + window_size])\n",
        "            next_word = tokens[i + window_size]\n",
        "            if current_state not in model:\n",
        "                model[current_state] = {}\n",
        "            if next_word not in model[current_state]:\n",
        "                model[current_state][next_word] = 0\n",
        "            model[current_state][next_word] += 1\n",
        "    return model\n",
        "\n",
        "def save_model_window(model, filepath):\n",
        "    \"\"\"\n",
        "    Saves the model dictionary to a JSON file, converting tuple keys to strings.\n",
        "    \"\"\"\n",
        "    print(f\"Saving model to {filepath}...\")\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    # Convert tuple keys to strings because JSON does not support tuple keys\n",
        "    string_keyed_model = {str(key): value for key, value in model.items()}\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        json.dump(string_keyed_model, f, indent=4)\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "def load_model_window(filepath):\n",
        "    \"\"\"\n",
        "    Loads the model from a JSON file, converting string keys back to tuples.\n",
        "    \"\"\"\n",
        "    print(f\"Loading model from {filepath}...\")\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        string_keyed_model = json.load(f)\n",
        "        # Convert string keys back to tuples for the model to work correctly\n",
        "        model = {ast.literal_eval(key): value for key, value in string_keyed_model.items()}\n",
        "    print(\"Model loaded successfully.\")\n",
        "    return model\n",
        "\n",
        "def generate_text_with_window(model, length=75, window_size=2, start_seed=None):\n",
        "    \"\"\"\n",
        "    Generates text from a windowed model, optionally starting with a given seed phrase.\n",
        "    \"\"\"\n",
        "    start_state = None\n",
        "\n",
        "    # Try to use the provided start_seed\n",
        "    if start_seed:\n",
        "        seed_words = start_seed.lower().split()\n",
        "        # Validate that the seed has the correct number of words\n",
        "        if len(seed_words) != window_size:\n",
        "            print(f\"Warning: Your start seed has {len(seed_words)} words, but the model's window size is {window_size}. Starting randomly.\")\n",
        "        else:\n",
        "            potential_start_state = tuple(seed_words)\n",
        "            # Validate that the seed exists as a state in our model\n",
        "            if potential_start_state in model:\n",
        "                start_state = potential_start_state\n",
        "            else:\n",
        "                print(f\"Warning: The phrase {potential_start_state} was not found in the model's training data. Starting randomly.\")\n",
        "\n",
        "    # If no seed was provided or the provided seed was invalid, start randomly\n",
        "    if start_state is None:\n",
        "        start_state = random.choice(list(model.keys()))\n",
        "\n",
        "    # --- The rest of the function remains the same ---\n",
        "    generated_text = list(start_state)\n",
        "    current_state = start_state\n",
        "    for _ in range(length - window_size):\n",
        "        if current_state not in model:\n",
        "            break\n",
        "        next_words_dict = model[current_state]\n",
        "        possible_next_words = list(next_words_dict.keys())\n",
        "        word_frequencies = list(next_words_dict.values())\n",
        "        chosen_next_word = random.choices(possible_next_words, weights=word_frequencies, k=1)[0]\n",
        "        generated_text.append(chosen_next_word)\n",
        "        current_state = tuple(generated_text[-window_size:])\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "# 1. Configuration\n",
        "WINDOW_SIZE = 2\n",
        "model_dir = \"/content/model/Markov_chain_v0.3\"\n",
        "# We'll make the filename dynamic based on the window size\n",
        "model_filename = f\"markov_model_ws{WINDOW_SIZE}.json\"\n",
        "model_filepath = os.path.join(model_dir, model_filename)\n",
        "\n",
        "# 2. Load or Build the Model\n",
        "if os.path.exists(model_filepath):\n",
        "    windowed_markov_model = load_model_window(model_filepath)\n",
        "else:\n",
        "    print(\"Saved model not found. Training a new one...\")\n",
        "    windowed_markov_model = build_markov_model_with_window(train, window_size=WINDOW_SIZE)\n",
        "    save_model_window(windowed_markov_model, model_filepath)\n",
        "\n",
        "# 3. Generate new text from the loaded/trained model\n",
        "USER_START_SEED = \"the sky\"\n",
        "\n",
        "# This line has been updated to pass the start_seed to the function\n",
        "new_text = generate_text_with_window(\n",
        "    windowed_markov_model,\n",
        "    length=75,\n",
        "    window_size=WINDOW_SIZE,\n",
        "    start_seed=USER_START_SEED\n",
        ")\n",
        "\n",
        "# 4. Print the result\n",
        "print(\"\\n--- Text Generated from Windowed Model ---\")\n",
        "print(new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RMaeZZYUIbO",
        "outputId": "ed703be6-226c-45b0-be06-569079049266"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model not found. Training a new one...\n",
            "Saving model to /content/model/Markov_chain_v0.3/markov_model_ws2.json...\n",
            "Model saved successfully.\n",
            "\n",
            "--- Text Generated from Windowed Model ---\n",
            "the sky , with a depth of 70 mph 110 km west of bir el mazar 42 miles 68 km east of the detroit metro times said coleman 's 1966 tune most likely abc 's radio presenting career , selected and edited excerpts of cruise ships operate from haifa . the street after an examination , and vertigo titles , in the latino community , creating a historical saga about the supernatural , much like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Configuration\n",
        "WINDOW_SIZE = 4\n",
        "model_dir = \"/content/model/Markov_chain_v0.3\"\n",
        "# We'll make the filename dynamic based on the window size\n",
        "model_filename = f\"markov_model_ws{WINDOW_SIZE}.json\"\n",
        "model_filepath = os.path.join(model_dir, model_filename)\n",
        "\n",
        "# 2. Load or Build the Model\n",
        "if os.path.exists(model_filepath):\n",
        "    windowed_markov_model = load_model_window(model_filepath)\n",
        "else:\n",
        "    print(\"Saved model not found. Training a new one...\")\n",
        "    windowed_markov_model = build_markov_model_with_window(train, window_size=WINDOW_SIZE)\n",
        "    save_model_window(windowed_markov_model, model_filepath)\n",
        "\n",
        "# 3. Generate new text from the loaded/trained model\n",
        "USER_START_SEED = \"\"\n",
        "\n",
        "# This line has been updated to pass the start_seed to the function\n",
        "new_text = generate_text_with_window(\n",
        "    windowed_markov_model,\n",
        "    length=150,\n",
        "    window_size=WINDOW_SIZE\n",
        "    #start_seed=USER_START_SEED\n",
        ")\n",
        "\n",
        "# 4. Print the result\n",
        "print(\"\\n--- Text Generated from Windowed Model ---\")\n",
        "print(new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL1mPG4eXpZC",
        "outputId": "cdf6019d-f8f0-4fd2-e5dd-12c05b3c4c4b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/model/Markov_chain_v0.3/markov_model_ws4.json...\n",
            "Model loaded successfully.\n",
            "\n",
            "--- Text Generated from Windowed Model ---\n",
            "only in 2004 that an original 35 mm film print was discovered due to the intervention of a fan .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Optimizations: Grammar and Memory\n",
        "\n",
        "In the next version of the model, we will perform two significant optimizations simultaneously to improve both the quality of the generated text and the model's efficiency.\n",
        "\n",
        "### 1. Teaching the Model Sentence Structure\n",
        "\n",
        "Currently, our model has no sense of the start and end of a sentence. A word at the end of a sentence and the same word in the middle are treated as completely different because of the attached punctuation.\n",
        "\n",
        "> For example, \"**found**\" and \"**found.**\" are two entirely separate words for our model.\n",
        "\n",
        "This is a major flaw, as the model never learns that a period signifies the end of a thought.\n",
        "\n",
        "We solve this by **treating punctuation as its own word (or \"token\")**. During preprocessing, we will pad spaces around major punctuation marks.\n",
        "\n",
        "* **Before:** `\"the cat sat found.\"`\n",
        "* **After:** The text is tokenized into `[\"the\", \"cat\", \"sat\", \"found\", \".\"]`\n",
        "\n",
        "This teaches the model the crucial relationship between words and punctuation, allowing it to learn how to properly end the sentences it generates.\n",
        "\n",
        "### 2. Reducing Memory Footprint with Integer Encoding\n",
        "\n",
        "Storing the entire model using full words (strings) is inefficient. Typically, storing one character requires one byte of memory, so a 7-character word like \"**awesome**\" uses at least 7 bytes. For a large corpus, the memory required for the model dictionary can become enormous.\n",
        "\n",
        "To solve this, we create a simple form of **word embedding** by mapping each unique word in our vocabulary to a unique integer.\n",
        "\n",
        "> For example: `{\"a\": 1, \"the\": 14, \"awesome\": 56, ...}`\n",
        "\n",
        "This fundamentally changes our model. Instead of a single large dictionary, our saved model now consists of **two essential components**:\n",
        "\n",
        "1.  **The Vocabulary Map:** A dictionary that maps words to their unique integer IDs (and another that maps IDs back to words for generation).\n",
        "2.  **The Core Model:** The main dictionary, which now stores these lightweight integers instead of heavy strings, dramatically reducing its size in memory.\n",
        "\n",
        "### 3. Decoupling Vocabulary from the Model for Experimentation\n",
        "\n",
        "Since we want to check the performance of our models for different window sizes, it is inefficient to store the vocabulary inside each model file. The vocabulary of the training text is constant; it does not change whether we use a window size of 2, 3, or 4.\n",
        "\n",
        "Therefore, we will adopt a more organized storage strategy:\n",
        "\n",
        "* **Vocabulary (`vocabulary.json`):** The word-to-integer mapping will be built once from the training data and saved to its own separate file.\n",
        "* **Model Dictionaries (`model_ws2.json`, `model_ws3.json`, etc.):** Each model, trained with a specific window size, will be saved to its own file, named dynamically based on its configuration. These model files will only contain the integer-based transition logic.\n",
        "\n",
        "This approach prevents data duplication, saves disk space, and keeps our experiments clean and organized. When we want to generate text, we will first load the single `vocabulary.json` and then load the specific `model_wsX.json` we wish to test.\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"vocabulary\": {\n",
        "    \"the\": 1,\n",
        "    \"sky\": 2,\n",
        "    \"is\": 3,\n",
        "    \"blue\": 4,\n",
        "    \"apple\": 5\n",
        "  },\n",
        "  \"model\": {\n",
        "    \"(2, 3)\": {\n",
        "      \"4\": 150\n",
        "    },\n",
        "    \"(1, 5)\": {\n",
        "      \"3\": 20\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "t62CRbubcNTj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "un0u_aaSZ-sy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}