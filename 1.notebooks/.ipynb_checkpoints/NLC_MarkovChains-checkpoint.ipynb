{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQbkSYL8DhPE"
   },
   "source": [
    "## Our Development Philosophy: From Baseline to Optimized\n",
    "\n",
    "This project follows a deliberate and iterative approach to model development. Our core principle is to ensure that every change is measured, understood, and contributes positively to the final result. This methodology transforms the development process from a series of guesses into a scientific experiment.\n",
    "\n",
    "### Phase 1: Build the Brute-Force Baseline\n",
    "\n",
    "First, we create the simplest possible, end-to-end working model. This **bare-minimum** version is not intended to be sophisticated; its purpose is to be a functional starting point that serves two critical purposes:\n",
    "\n",
    "1.  **Proof of Concept:** It confirms that our data pipeline, from loading to processing and modeling, is functional. It validates our foundational assumptions.\n",
    "2.  **Establish a Benchmark:** It provides a clear **baseline performance metric**. All future work will be measured against this initial score. Without a baseline, it's impossible to quantify improvement.\n",
    "\n",
    "### Phase 2: Optimize One Step at a Time\n",
    "\n",
    "Once the baseline is established, we begin a cycle of incremental improvement. We strictly adhere to the principle of making **one isolated change at a time**. This is the most critical aspect of our philosophy.\n",
    "\n",
    "Instead of overhauling the model at once and being unable to pinpoint the source of a change, we will:\n",
    "\n",
    "* **Target** a single component for improvement (e.g., memory usage, context awareness, grammar).\n",
    "* **Implement** that one specific change.\n",
    "* **Measure** its exact impact on performance and resource consumption.\n",
    "\n",
    "This methodical process removes guesswork and allows us to attribute performance gains or losses directly to a specific action, ensuring that every step forward is a confident one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading the Dataset\n",
    "\n",
    "Our journey begins by loading the `wikitext` dataset, a large corpus of text derived from high-quality Wikipedia articles. We use the `datasets` library from Hugging Face for this task, which provides a standardized and efficient way to handle large datasets.\n",
    "\n",
    "We also specify a custom cache directory (`../2.data`) to ensure that the downloaded data is stored in a predictable location within our project structure, making it easier to manage and reuse without re-downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSS3p4etDy-C",
    "outputId": "8da2a36b-12c7-4301-a702-efcf9010f430"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jamin Carter\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define a custom directory to cache the downloaded dataset\n",
    "my_custom_cache_dir = \"../2.data\"\n",
    "\n",
    "# Load the 'wikitext-2-raw-v1' version of the wikitext dataset\n",
    "wikitext_dataset = load_dataset(\n",
    "    \"wikitext\",\n",
    "    \"wikitext-2-raw-v1\",\n",
    "    cache_dir=my_custom_cache_dir\n",
    ")\n",
    "\n",
    "# Print the dataset structure to see the different splits (train, validation, test)\n",
    "print(wikitext_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initial Data Exploration\n",
    "\n",
    "Before we can clean the data, we need to understand its raw form. We'll inspect the first few lines of the training set to identify patterns, noise, and artifacts that need to be addressed in our preprocessing stage. This step is crucial for formulating an effective cleaning strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7eMZxqOFVyA",
    "outputId": "0ec87883-ac4a-4b1e-9fff-fbe96156146f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\n",
      "1: = Valkyria Chronicles III = \n",
      "\n",
      "2:\n",
      "3: Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      "\n",
      "4: The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create separate variables for easier access to each dataset split\n",
    "test_data = wikitext_dataset['test']\n",
    "train_data = wikitext_dataset['train']\n",
    "validation_data = wikitext_dataset['validation']\n",
    "\n",
    "# Print the first 5 lines of the training data to inspect its content\n",
    "for i in range(5):\n",
    "  print(f\"{i}:{train_data[i]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKOkr78xHxzu"
   },
   "source": [
    "### Step 3: Defining the Preprocessing Strategy\n",
    "\n",
    "Based on our initial exploration, we have identified several types of noise in the raw text. To prepare the data for our model, we will implement a multi-step cleaning process:\n",
    "\n",
    "* **Remove Article Headings:** The dataset uses lines starting with ` = ` to denote section titles (e.g., `= = Gameplay = =`). These are metadata, not narrative text, and should be removed.\n",
    "* **Handle Special Characters:** The text contains artifacts like `@-@` and non-ASCII characters (e.g., `戦場のヴァルキュリア3`). We will remove these to create a cleaner, more consistent vocabulary.\n",
    "* **Standardize Casing:** All text will be converted to lowercase to ensure that the model treats the same word (e.g., \"The\" and \"the\") as a single entity.\n",
    "* **Remove Empty Lines:** The dataset contains many blank lines, which add no value and should be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Measuring the Impact of Preprocessing\n",
    "\n",
    "Before applying our cleaning functions, we will establish a baseline measurement of the dataset's size. We'll calculate the total number of rows and characters in each split (train, validation, and test). After preprocessing, we will perform the same calculation again. This allows us to quantify the exact impact of our cleaning process, showing how much noise we have successfully removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "li5oyf7PGZ47",
    "outputId": "fd6be37e-0789-496a-d35d-96bd6135b833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before Cleaning ---\n",
      "Training Data:   36,718 rows, 10,892,990 characters\n",
      "Validation Data: 3,760 rows, 1,142,150 characters\n",
      "Test Data:       4,358 rows, 1,285,622 characters\n",
      "\n",
      "--- After Cleaning ---\n",
      "Training Data:   23,764 rows, 10,617,709 characters\n",
      "Validation Data: 2,461 rows, 1,114,644 characters\n",
      "Test Data:       2,891 rows, 1,253,140 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# --- Before Preprocessing ---\n",
    "print(\"--- Before Cleaning ---\")\n",
    "# Calculate and print the initial size of each dataset split\n",
    "train_chars_before = sum(len(line) for line in train_data['text'] if line)\n",
    "validation_chars_before = sum(len(line) for line in validation_data['text'] if line)\n",
    "test_chars_before = sum(len(line) for line in test_data['text'] if line)\n",
    "print(f\"Training Data:   {train_data.num_rows:,} rows, {train_chars_before:,} characters\")\n",
    "print(f\"Validation Data: {validation_data.num_rows:,} rows, {validation_chars_before:,} characters\")\n",
    "print(f\"Test Data:       {test_data.num_rows:,} rows, {test_chars_before:,} characters\")\n",
    "\n",
    "# --- Preprocessing Steps ---\n",
    "# The 'datasets' library allows us to chain operations for a clean pipeline.\n",
    "\n",
    "# 1. Filter out article headings\n",
    "processed_dataset = wikitext_dataset.filter(\n",
    "    lambda example: not example['text'].strip().startswith(' = ')\n",
    ")\n",
    "\n",
    "# 2. Apply text cleaning and normalization to each entry\n",
    "processed_dataset = processed_dataset.map(\n",
    "    lambda example: {\n",
    "        'text': re.sub(\n",
    "            r'[^a-zA-Z0-9\\s.,\\'?!-]', '', # Keep only alphanumeric, common punctuation, and whitespace\n",
    "            example['text'].lower().replace('@-@', '') # Convert to lowercase and remove artifacts\n",
    "        ).strip()\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Filter out any lines that became empty after cleaning\n",
    "processed_dataset = processed_dataset.filter(\n",
    "    lambda example: len(example['text']) > 0\n",
    ")\n",
    "\n",
    "# --- After Preprocessing ---\n",
    "print(\"\\n--- After Cleaning ---\")\n",
    "# Re-assign the cleaned data to our variables for future use\n",
    "train_data = processed_dataset['train']\n",
    "validation_data = processed_dataset['validation']\n",
    "test_data = processed_dataset['test']\n",
    "\n",
    "# Calculate and print the final size of each dataset split\n",
    "train_chars_after = sum(len(line) for line in train_data['text'] if line)\n",
    "validation_chars_after = sum(len(line) for line in validation_data['text'] if line)\n",
    "test_chars_after = sum(len(line) for line in test_data['text'] if line)\n",
    "print(f\"Training Data:   {train_data.num_rows:,} rows, {train_chars_after:,} characters\")\n",
    "print(f\"Validation Data: {validation_data.num_rows:,} rows, {validation_chars_after:,} characters\")\n",
    "print(f\"Test Data:       {test_data.num_rows:,} rows, {test_chars_after:,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Verifying the Cleaned Data\n",
    "\n",
    "After preprocessing, it's essential to visually inspect the data again. This final check confirms that our cleaning functions worked as expected and that the text is now in a suitable format for our model. We expect to see clean, lowercase text with no strange artifacts or article headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCH8m9xVK52I",
    "outputId": "04824f3d-eeb7-4240-ab41-1aba1c299fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:valkyria chronicles iii\n",
      "1:senj no valkyria 3  unrecorded chronicles  japanese  3 , lit . valkyria of the battlefield 3  , commonly referred to as valkyria chronicles iii outside japan , is a tactical role  playing video game developed by sega and media.vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . employing the same fusion of tactical and real  time gameplay as its predecessors , the story runs parallel to the first game and follows the  nameless  , a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit  calamaty raven  .\n",
      "2:the game began development in 2010 , carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . character designer raita honjou and composer hitoshi sakimoto both returned from previous entries , along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game 's opening theme was sung by may 'n .\n",
      "3:it met with positive sales in japan , and was praised by both japanese and western critics . after release , it received downloadable content , along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . media.vision would return to the franchise with the development of valkyria  azure revolution for the playstation 4 .\n",
      "4:gameplay\n"
     ]
    }
   ],
   "source": [
    "# Visually inspect the first 20 lines of the cleaned training data\n",
    "for i in range(5):\n",
    "  print(f\"{i}:{train_data[i]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIJMdwzPN5_P"
   },
   "source": [
    "## Baseline Model: The Simple Markov Chain\n",
    "\n",
    "Our first model is a foundational, first-order Markov chain. It's a simple probabilistic model that predicts the next word in a sequence based *solely* on the current word, ignoring all previous context.\n",
    "\n",
    "### What is \"The Model\"?\n",
    "\n",
    "In this context, the \"model\" isn't a complex algorithm but a simple and intuitive data structure: a **Python dictionary**. This dictionary is the final output of the \"training\" process. It stores all the learned word-to-word transition probabilities from the input text.\n",
    "\n",
    "### The Structure of the Model (The Dictionary)\n",
    "\n",
    "The model is a nested dictionary with a specific structure: `word -> {next_word: count}`.\n",
    "\n",
    "* **Level 1: The Keys (Current Words)**\n",
    "    * The keys of the main dictionary are all the unique words found in the text. Each key represents a possible \"current state.\"\n",
    "* **Level 2: The Values (Next Words and Their Frequencies)**\n",
    "    * The value associated with each key is *another dictionary*.\n",
    "    * In this inner dictionary, the keys are all the words that have ever appeared immediately after the \"current word.\"\n",
    "    * The values are the counts (integers) of how many times that specific transition occurred.\n",
    "\n",
    "### A Concrete Example\n",
    "\n",
    "If our training text is: `\"the cat sat on the mat\"`\n",
    "\n",
    "The resulting model dictionary would look like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"the\": {\n",
    "        \"cat\": 1,\n",
    "        \"mat\": 1\n",
    "    },\n",
    "    \"cat\": {\n",
    "        \"sat\": 1\n",
    "    },\n",
    "    \"sat\": {\n",
    "        \"on\": 1\n",
    "    },\n",
    "    \"on\": {\n",
    "        \"the\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDXONapyNFhX",
    "outputId": "82e77f97-aa74-4760-bc30-befeb8e9b3c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training corpus...\n",
      "Corpus prepared.\n",
      "Building Markov model...\n",
      "Loading saved model...\n",
      "\n",
      "--- Text Generated from WikiText Model ---\n",
      "morning , which initiated a favorite soul in uninhabited peninsula . it is marked with other than 100 , such as the body drop of boosted song abdicated and buddhism here drains one car dealership , where are prevalent topic of the late as the road crossing the group of leinster . the rard pedal piano part of henry iii connor chosen for the dominican airports on the dandenong rangers hall , the public and\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import json\n",
    "\n",
    "\n",
    "def build_markov_model(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Builds a simple Markov chain model from a given text.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "\n",
    "    model = {}\n",
    "\n",
    "    for i in range(len(tokens) - 1):\n",
    "        current_word = tokens[i]\n",
    "        next_word = tokens[i + 1]\n",
    "\n",
    "        if current_word not in model:\n",
    "            model[current_word] = {}\n",
    "\n",
    "        if next_word not in model[current_word]:\n",
    "            model[current_word][next_word] = 0\n",
    "\n",
    "        model[current_word][next_word] += 1\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_text(model: dict, length: int = 50,start: str=None) -> str:\n",
    "    if start is None:\n",
    "      start_word=random.choice(list(model.keys()))\n",
    "    else:\n",
    "        start_word=start.lower()\n",
    "    \"\"\"\n",
    "    Generates new text using a pre-built Markov model.\n",
    "    \"\"\"\n",
    "    generated_text = [start_word]\n",
    "    current_word = start_word\n",
    "\n",
    "    for _ in range(length - 1):\n",
    "        if current_word not in model:\n",
    "            break\n",
    "\n",
    "        next_words_dict = model[current_word]\n",
    "        possible_next_words = list(next_words_dict.keys())\n",
    "        word_frequencies = list(next_words_dict.values())\n",
    "\n",
    "        chosen_next_word = random.choices(possible_next_words, weights=word_frequencies, k=1)[0]\n",
    "\n",
    "        generated_text.append(chosen_next_word)\n",
    "        current_word = chosen_next_word\n",
    "\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "\n",
    "# Helper Functions Ignore them\n",
    "import os\n",
    "model_filepath = \"../3.model/Markov_chain_v0.1\" # <-- Change this path\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    \"\"\"Saves the model dictionary to a JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(model, f)\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"Loads the model dictionary from a JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# --- Main execution ---\n",
    "\n",
    "# 1. Convert the 'train' dataset object into a single corpus string\n",
    "# We join all the lines of text together with spaces in between.\n",
    "print(\"Preparing training corpus...\")\n",
    "corpus = \" \".join(train_data['text'])\n",
    "print(\"Corpus prepared.\")\n",
    "\n",
    "# 2. Build the model using the training corpus\n",
    "print(\"Building Markov model...\")\n",
    "if os.path.exists(model_filepath):\n",
    "    print(\"Loading saved model...\")\n",
    "    markov_model = load_model(model_filepath)\n",
    "else:\n",
    "    print(\"No saved model found. Building a new one...\")\n",
    "    markov_model = build_markov_model(corpus)\n",
    "    save_model(markov_model, model_filepath)\n",
    "\n",
    "\n",
    "# 3. Generate new text from the trained model\n",
    "new_text = generate_text(markov_model, length=75,start=\"morning\")\n",
    "\n",
    "# 4. Print the result\n",
    "print(\"\\n--- Text Generated from WikiText Model ---\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5fri8tWSehW"
   },
   "source": [
    "## Optimization 1: Memory Efficiency\n",
    "\n",
    "The initial brute-force approach required loading the entire training dataset into a **single, massive string**. This is highly problematic for large datasets for two main reasons:\n",
    "\n",
    "1.  **High Memory Usage:** It can consume several gigabytes of RAM, potentially crashing the program with a `MemoryError` on systems with limited memory.\n",
    "2.  **Loss of Context:** It destroys natural sentence boundaries, leading to a lower-quality model that can't learn to start or end sentences properly.\n",
    "\n",
    "To solve this, we **process the data incrementally**. Instead of creating one large object in memory, the optimized approach is to build the model **line-by-line**.\n",
    "\n",
    "We iterate through the dataset, processing only one line at a time to update our model dictionary. This is a highly **memory-efficient** solution, as it allows us to train on a dataset of virtually any size while using a nearly constant amount of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcuTb1jaOoc3",
    "outputId": "12cf38a6-9990-4f90-a9f3-71d66816e3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model...\n",
      "Model is ready.\n",
      "\n",
      "--- Generating new text ---\n",
      "\n",
      "--- Generated Text ---\n",
      "morning of the clemson university crews were transferred to include an attempt a pound 50 and accused him off the dc 4 . eva pern . in 1958 , the episode for sea in some time , the halo 2 tacklers were prevented from the numbers to the only cable network five albums chart on october 5 , coins made on the eve myles and removed , a half of the councils , known as\n"
     ]
    }
   ],
   "source": [
    "def build_markov_model_incrementally(dataset_split):\n",
    "    \"\"\"\n",
    "    Builds a model by iterating through the dataset line by line (memory-efficient).\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    for line in dataset_split['text']:\n",
    "        tokens = line.split()\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        for i in range(len(tokens) - 1):\n",
    "            current_word = tokens[i]\n",
    "            next_word = tokens[i + 1]\n",
    "\n",
    "            if current_word not in model:\n",
    "                model[current_word] = {}\n",
    "\n",
    "            if next_word not in model[current_word]:\n",
    "                model[current_word][next_word] = 0\n",
    "\n",
    "            model[current_word][next_word] += 1\n",
    "\n",
    "    return model\n",
    "# --- Main Execution (Incremental Approach) ---\n",
    "# Assumes 'train' dataset object and all helper functions are pre-defined.\n",
    "\n",
    "# 1. Configuration\n",
    "MODEL_FILEPATH = \"../3.model/Markov_chain_v0.2\"\n",
    "START_WORD = \"morning\"\n",
    "TEXT_LENGTH = 75\n",
    "\n",
    "# 2. Load or Build the Model Incrementally\n",
    "if os.path.exists(MODEL_FILEPATH):\n",
    "    print(\"Loading existing model...\")\n",
    "    markov_model = load_model(MODEL_FILEPATH)\n",
    "else:\n",
    "    print(\"No existing model found. Building a new one incrementally...\")\n",
    "\n",
    "    # This is the corrected, memory-efficient call.\n",
    "    # It passes the dataset object directly to the incremental builder.\n",
    "    markov_model = build_markov_model_incrementally(train_data)\n",
    "\n",
    "    save_model(markov_model, MODEL_FILEPATH)\n",
    "\n",
    "print(\"Model is ready.\")\n",
    "\n",
    "# 3. Generate and Print Text\n",
    "print(\"\\n--- Generating new text ---\")\n",
    "generated_text = generate_text(\n",
    "    markov_model,\n",
    "    length=TEXT_LENGTH,\n",
    "    start=START_WORD\n",
    ")\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9Ng2W3AUw3G"
   },
   "source": [
    "## Optimization 2: Increasing Context with a Larger Window\n",
    "\n",
    "The most significant weakness of our current model is its extremely limited memory. It is a **first-order Markov chain**, meaning it only knows the previous **1** word. This leads to a critical failure in logical consistency.\n",
    "\n",
    "### The Problem of Lost Context\n",
    "\n",
    "Consider the phrase: `\"The sky is...\"`\n",
    "\n",
    "Our model's decision for the next word is based *only* on the word `\"is\"`. It has forgotten the crucial context word `\"sky\"`. The model's pool of possible next words is created from *every single instance* where `\"is\"` appeared in the training text, regardless of what came before it. It learns from unrelated phrases like:\n",
    "\n",
    "* `\"...the sky is blue...\"`\n",
    "* `\"...the company's logo is an apple...\"`\n",
    "\n",
    "When the model's only context is `\"is\"`, it blends these unrelated scenarios. It might correctly identify `\"blue\"` as probable, but `\"apple\"` remains a possibility because the model has no memory of the preceding word, `\"sky\"`.\n",
    "\n",
    "### The Solution: Using Tuples for State\n",
    "\n",
    "To solve this, we increase the model's memory by using a **tuple of multiple words** as the key. This tuple acts as our new \"state,\" effectively filtering out irrelevant choices.\n",
    "\n",
    "* **Old Model State (Plain Word):** `is`\n",
    "    * Possible Next Words: `{ \"apple\": 3, \"blue\": 4, \"green\": 2, ... }`\n",
    "* **New Model State (Tuple):** `('sky', 'is')`\n",
    "    * Possible Next Words: `{ \"blue\": 150, \"clear\": 80, \"overcast\": 45, ... }`\n",
    "\n",
    "By using a tuple, the model's state is now `('sky', 'is')`. The pool of possible next words it considers is now completely different and far more relevant. The word `\"apple\"` is unlikely to even be in this new list, leading to much more coherent and context-aware text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RMaeZZYUIbO",
    "outputId": "ed703be6-226c-45b0-be06-569079049266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../3.model/Markov_chain_v0.3\\markov_model_ws2.json...\n",
      "Model loaded successfully.\n",
      "\n",
      "--- Text Generated from Windowed Model ---\n",
      "the sky due to the admiralty insisting that badrinath is dissimilar to parasaurolophus in that game 's puzzles . in the later importance of being attached to the lasing medium . as egyptian society . prasad says that grisham returned to the team endured their first major car updates for the second and third best playstation portable consoles . live action film adaptation of hellblazer ever filmed for the chapters were collected into a diplomatic\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import ast # Used to safely convert string representations of tuples back to tuples\n",
    "\n",
    "# (Assuming 'processed_dataset' is already created and split into train, validation, test)\n",
    "# train = processed_dataset['train']\n",
    "\n",
    "def build_markov_model_with_window(dataset_split, window_size=2):\n",
    "    \"\"\"\n",
    "    Builds a higher-order Markov model using a tuple of words (the \"window\") as the state.\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    for line in dataset_split['text']:\n",
    "        tokens = line.split()\n",
    "        if len(tokens) < window_size + 1:\n",
    "            continue\n",
    "        for i in range(len(tokens) - window_size):\n",
    "            current_state = tuple(tokens[i : i + window_size])\n",
    "            next_word = tokens[i + window_size]\n",
    "            if current_state not in model:\n",
    "                model[current_state] = {}\n",
    "            if next_word not in model[current_state]:\n",
    "                model[current_state][next_word] = 0\n",
    "            model[current_state][next_word] += 1\n",
    "    return model\n",
    "\n",
    "def save_model_window(model, filepath):\n",
    "    \"\"\"\n",
    "    Saves the model dictionary to a JSON file, converting tuple keys to strings.\n",
    "    \"\"\"\n",
    "    print(f\"Saving model to {filepath}...\")\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    # Convert tuple keys to strings because JSON does not support tuple keys\n",
    "    string_keyed_model = {str(key): value for key, value in model.items()}\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(string_keyed_model, f, indent=4)\n",
    "    print(\"Model saved successfully.\")\n",
    "\n",
    "def load_model_window(filepath):\n",
    "    \"\"\"\n",
    "    Loads the model from a JSON file, converting string keys back to tuples.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {filepath}...\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        string_keyed_model = json.load(f)\n",
    "        # Convert string keys back to tuples for the model to work correctly\n",
    "        model = {ast.literal_eval(key): value for key, value in string_keyed_model.items()}\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "def generate_text_with_window(model, length=75, window_size=2, start_seed=None):\n",
    "    \"\"\"\n",
    "    Generates text from a windowed model, optionally starting with a given seed phrase.\n",
    "    \"\"\"\n",
    "    start_state = None\n",
    "\n",
    "    # Try to use the provided start_seed\n",
    "    if start_seed:\n",
    "        seed_words = start_seed.lower().split()\n",
    "        # Validate that the seed has the correct number of words\n",
    "        if len(seed_words) != window_size:\n",
    "            print(f\"Warning: Your start seed has {len(seed_words)} words, but the model's window size is {window_size}. Starting randomly.\")\n",
    "        else:\n",
    "            potential_start_state = tuple(seed_words)\n",
    "            # Validate that the seed exists as a state in our model\n",
    "            if potential_start_state in model:\n",
    "                start_state = potential_start_state\n",
    "            else:\n",
    "                print(f\"Warning: The phrase {potential_start_state} was not found in the model's training data. Starting randomly.\")\n",
    "\n",
    "    # If no seed was provided or the provided seed was invalid, start randomly\n",
    "    if start_state is None:\n",
    "        start_state = random.choice(list(model.keys()))\n",
    "\n",
    "    # --- The rest of the function remains the same ---\n",
    "    generated_text = list(start_state)\n",
    "    current_state = start_state\n",
    "    for _ in range(length - window_size):\n",
    "        if current_state not in model:\n",
    "            break\n",
    "        next_words_dict = model[current_state]\n",
    "        possible_next_words = list(next_words_dict.keys())\n",
    "        word_frequencies = list(next_words_dict.values())\n",
    "        chosen_next_word = random.choices(possible_next_words, weights=word_frequencies, k=1)[0]\n",
    "        generated_text.append(chosen_next_word)\n",
    "        current_state = tuple(generated_text[-window_size:])\n",
    "\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# 1. Configuration\n",
    "WINDOW_SIZE = 2\n",
    "model_dir = \"../3.model/Markov_chain_v0.3\"\n",
    "# We'll make the filename dynamic based on the window size\n",
    "model_filename = f\"markov_model_ws{WINDOW_SIZE}.json\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "# 2. Load or Build the Model\n",
    "if os.path.exists(model_filepath):\n",
    "    windowed_markov_model = load_model_window(model_filepath)\n",
    "else:\n",
    "    print(\"Saved model not found. Training a new one...\")\n",
    "    windowed_markov_model = build_markov_model_with_window(train_data, window_size=WINDOW_SIZE)\n",
    "    save_model_window(windowed_markov_model, model_filepath)\n",
    "\n",
    "# 3. Generate new text from the loaded/trained model\n",
    "USER_START_SEED = \"the sky\"\n",
    "\n",
    "# This line has been updated to pass the start_seed to the function\n",
    "new_text = generate_text_with_window(\n",
    "    windowed_markov_model,\n",
    "    length=75,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    start_seed=USER_START_SEED\n",
    ")\n",
    "\n",
    "# 4. Print the result\n",
    "print(\"\\n--- Text Generated from Windowed Model ---\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WL1mPG4eXpZC",
    "outputId": "cdf6019d-f8f0-4fd2-e5dd-12c05b3c4c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../3.model/Markov_chain_v0.3\\markov_model_ws4.json...\n",
      "Model loaded successfully.\n",
      "\n",
      "--- Text Generated from Windowed Model ---\n",
      "reform that were central to their struggle .\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuration\n",
    "WINDOW_SIZE = 4\n",
    "model_dir = \"../3.model/Markov_chain_v0.3\"\n",
    "# We'll make the filename dynamic based on the window size\n",
    "model_filename = f\"markov_model_ws{WINDOW_SIZE}.json\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "# 2. Load or Build the Model\n",
    "if os.path.exists(model_filepath):\n",
    "    windowed_markov_model = load_model_window(model_filepath)\n",
    "else:\n",
    "    print(\"Saved model not found. Training a new one...\")\n",
    "    windowed_markov_model = build_markov_model_with_window(train_data, window_size=WINDOW_SIZE)\n",
    "    save_model_window(windowed_markov_model, model_filepath)\n",
    "\n",
    "# 3. Generate new text from the loaded/trained model\n",
    "USER_START_SEED = \"\"\n",
    "\n",
    "# This line has been updated to pass the start_seed to the function\n",
    "new_text = generate_text_with_window(\n",
    "    windowed_markov_model,\n",
    "    length=150,\n",
    "    window_size=WINDOW_SIZE\n",
    "    #start_seed=USER_START_SEED\n",
    ")\n",
    "\n",
    "# 4. Print the result\n",
    "print(\"\\n--- Text Generated from Windowed Model ---\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t62CRbubcNTj"
   },
   "source": [
    "## Final Optimizations: Grammar and Memory\n",
    "\n",
    "In the final version, we will perform two significant optimizations simultaneously to improve both the quality of the generated text and the model's efficiency.\n",
    "\n",
    "### 1. Teaching the Model Sentence Structure\n",
    "\n",
    "Currently, our model has no sense of the start and end of a sentence. A word at the end of a sentence and the same word in the middle are treated as completely different because of the attached punctuation.\n",
    "\n",
    "> For example, `\"found\"` and `\"found.\"` are two entirely separate words for our model.\n",
    "\n",
    "This is a major flaw, as the model never learns that a period signifies the end of a thought. We solve this by **treating punctuation as its own word (or \"token\")**. During preprocessing, we will pad spaces around major punctuation marks.\n",
    "\n",
    "* **Before:** `\"the cat sat found.\"`\n",
    "* **After:** The text is tokenized into `[\"the\", \"cat\", \"sat\", \"found\", \".\"]`\n",
    "\n",
    "This teaches the model the crucial relationship between words and punctuation, allowing it to learn how to properly end the sentences it generates.\n",
    "\n",
    "### 2. Reducing Memory Footprint with Integer Encoding\n",
    "\n",
    "Storing the entire model using full words (strings) is inefficient. Typically, storing one character requires one byte of memory, so a 7-character word like `\"awesome\"` uses at least 7 bytes. For a large corpus, the memory required for the model dictionary can become enormous.\n",
    "\n",
    "To solve this, we create a simple form of **tokenization** by mapping each unique word in our vocabulary to a unique integer.\n",
    "\n",
    "> For example: `{\"a\": 1, \"the\": 14, \"awesome\": 56, ...}`\n",
    "\n",
    "This fundamentally changes our model. Instead of a single large dictionary, our saved model now consists of **two essential components**:\n",
    "\n",
    "1.  **The Vocabulary Maps:** Two dictionaries, one that maps words to their unique integer IDs (`word_to_int`) and another that maps IDs back to words (`int_to_word`) for generation.\n",
    "2.  **The Core Model:** The main dictionary, which now stores these lightweight integers instead of heavy strings, dramatically reducing its size in memory and on disk.\n",
    "\n",
    "### 3. Decoupling Vocabulary from the Model for Experimentation\n",
    "\n",
    "Since we want to check the performance of our models for different window sizes, it is inefficient to store the vocabulary inside each model file. The vocabulary of the training text is constant; it does not change whether we use a window size of 2, 3, or 4.\n",
    "\n",
    "Therefore, we will adopt a more organized storage strategy:\n",
    "\n",
    "* **Vocabulary (`vocabulary.json`):** The word-to-integer mappings will be built once from the training data and saved to their own separate file.\n",
    "* **Model Dictionaries (`int_model_ws2.json`, etc.):** Each model, trained with a specific window size, will be saved to its own file, named dynamically based on its configuration. These model files will only contain the integer-based transition logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "un0u_aaSZ-sy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vocabulary...\n",
      "Loading final model with window size 3...\n",
      "Loading model from ../3.model/Markov_chain_v0.4\\int_model_ws3.json...\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "malformed node or string: (3618, 3212, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 151\u001b[39m\n\u001b[32m    148\u001b[39m     final_model = {\u001b[38;5;28mstr\u001b[39m(k): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m final_model_tuples.items()}\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# 3. Generate Text\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m generated_text = \u001b[43mgenerate_text_integer_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mint_to_word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mword_to_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWINDOW_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthe history of\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    158\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Text Generated from Final Model ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    161\u001b[39m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mgenerate_text_integer_model\u001b[39m\u001b[34m(model, int_to_word, word_to_int, length, window_size, start_seed)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# If no valid seed is found, pick a random starting state from the model keys\u001b[39;00m\n\u001b[32m     92\u001b[39m     random_start_key = random.choice(\u001b[38;5;28mlist\u001b[39m(model.keys()))\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     start_state = \u001b[43mast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_start_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m generated_ids = \u001b[38;5;28mlist\u001b[39m(start_state)\n\u001b[32m     96\u001b[39m current_state_tuple = start_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:110\u001b[39m, in \u001b[36mliteral_eval\u001b[39m\u001b[34m(node_or_string)\u001b[39m\n\u001b[32m    108\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m left - right\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:109\u001b[39m, in \u001b[36mliteral_eval.<locals>._convert\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    107\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    108\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m left - right\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:83\u001b[39m, in \u001b[36mliteral_eval.<locals>._convert_signed_num\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     82\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m - operand\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:74\u001b[39m, in \u001b[36mliteral_eval.<locals>._convert_num\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_num\u001b[39m(node):\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node.value) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m node.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:71\u001b[39m, in \u001b[36mliteral_eval.<locals>._raise_malformed_node\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lno := \u001b[38;5;28mgetattr\u001b[39m(node, \u001b[33m'\u001b[39m\u001b[33mlineno\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     70\u001b[39m     msg += \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlno\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg + \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: malformed node or string: (3618, 3212, 9)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import ast\n",
    "\n",
    "# --- Step 1: Grammar-Aware Preprocessing and Vocabulary Building ---\n",
    "\n",
    "def tokenize_and_build_vocab(dataset_split):\n",
    "    \"\"\"\n",
    "    Processes text to separate punctuation and builds word-to-integer mappings.\n",
    "    This function creates the foundation for our grammar-aware, memory-efficient model.\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    processed_lines = []\n",
    "\n",
    "    for line in dataset_split['text']:\n",
    "        # Use regex to add spaces around major punctuation, treating them as separate tokens.\n",
    "        # This is the key to teaching the model sentence structure.\n",
    "        line = re.sub(r'([.,?!])', r' \\1 ', line)\n",
    "        tokens = line.split()\n",
    "        processed_lines.append(tokens)\n",
    "        \n",
    "        # Count the frequency of each unique token (word or punctuation)\n",
    "        for token in tokens:\n",
    "            word_counts[token] = word_counts.get(token, 0) + 1\n",
    "            \n",
    "    # Create the vocabulary mappings.\n",
    "    # We sort words by frequency; this is optional but good practice.\n",
    "    sorted_words = sorted(word_counts.keys(), key=lambda x: word_counts[x], reverse=True)\n",
    "    word_to_int = {word: i for i, word in enumerate(sorted_words)}\n",
    "    int_to_word = {i: word for i, word in enumerate(sorted_words)}\n",
    "    \n",
    "    return processed_lines, word_to_int, int_to_word\n",
    "\n",
    "def save_vocab(word_to_int, int_to_word, filepath):\n",
    "    \"\"\"Saves the vocabulary maps to a single JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\"word_to_int\": word_to_int, \"int_to_word\": int_to_word}, f, indent=4)\n",
    "\n",
    "def load_vocab(filepath):\n",
    "    \"\"\"Loads the vocabulary maps from a JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # The int_to_word map gets its keys converted to strings by JSON, so we fix them back to integers.\n",
    "    int_to_word_corrected = {int(k): v for k, v in data['int_to_word'].items()}\n",
    "    return data['word_to_int'], int_to_word_corrected\n",
    "\n",
    "# --- Step 2: Build the Integer-Based Markov Model ---\n",
    "\n",
    "# CORRECTED function\n",
    "def build_integer_model(processed_lines, word_to_int, window_size=2):\n",
    "    \"\"\"\n",
    "    Builds a windowed Markov model using STRING keys for consistency.\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    for tokens in processed_lines:\n",
    "        if len(tokens) < window_size + 1:\n",
    "            continue\n",
    "        int_tokens = [word_to_int[token] for token in tokens if token in word_to_int]\n",
    "        \n",
    "        for i in range(len(int_tokens) - window_size):\n",
    "            # Create the tuple state...\n",
    "            current_state_tuple = tuple(int_tokens[i : i + window_size])\n",
    "            # ...and immediately convert it to a string for use as a key.\n",
    "            current_state_str = str(current_state_tuple)\n",
    "            next_word_id = int_tokens[i + window_size]\n",
    "            \n",
    "            # Now, use the string key for all dictionary operations.\n",
    "            if current_state_str not in model:\n",
    "                model[current_state_str] = {}\n",
    "            \n",
    "            next_word_id_str = str(next_word_id)\n",
    "            if next_word_id_str not in model[current_state_str]:\n",
    "                model[current_state_str][next_word_id_str] = 0\n",
    "            model[current_state_str][next_word_id_str] += 1\n",
    "            \n",
    "    return model # This model now has string keys by default.\n",
    "# --- Step 3: Text Generation with Integer Model ---\n",
    "\n",
    "def generate_text_integer_model(model, int_to_word, word_to_int, length=75, window_size=2, start_seed=None):\n",
    "    \"\"\"\n",
    "    Generates text from an integer-based model, then decodes it back to words.\n",
    "    \"\"\"\n",
    "    start_state = None\n",
    "    if start_seed:\n",
    "        seed_words = start_seed.lower().split()\n",
    "        if len(seed_words) == window_size and all(word in word_to_int for word in seed_words):\n",
    "            potential_start_state = tuple(word_to_int[word] for word in seed_words)\n",
    "            if str(potential_start_state) in model:\n",
    "                start_state = potential_start_state\n",
    "\n",
    "    if start_state is None:\n",
    "        # If no valid seed is found, pick a random starting state from the model keys\n",
    "        random_start_key = random.choice(list(model.keys()))\n",
    "        start_state = ast.literal_eval(random_start_key)\n",
    "\n",
    "    generated_ids = list(start_state)\n",
    "    current_state_tuple = start_state\n",
    "\n",
    "    for _ in range(length - window_size):\n",
    "        current_state_str = str(current_state_tuple)\n",
    "        if current_state_str not in model:\n",
    "            break\n",
    "        \n",
    "        next_ids_dict = model[current_state_str]\n",
    "        possible_next_ids_str = list(next_ids_dict.keys())\n",
    "        id_frequencies = list(next_ids_dict.values())\n",
    "        \n",
    "        # JSON keys are strings, so we convert them to integers for the random choice\n",
    "        chosen_next_id = random.choices([int(i) for i in possible_next_ids_str], weights=id_frequencies, k=1)[0]\n",
    "        generated_ids.append(chosen_next_id)\n",
    "        current_state_tuple = tuple(generated_ids[-window_size:])\n",
    "\n",
    "    # Decode the sequence of integer IDs back into words\n",
    "    generated_words = [int_to_word.get(id, '?') for id in generated_ids]\n",
    "    \n",
    "    # Post-process the text for better readability (joining punctuation correctly)\n",
    "    return ' '.join(generated_words).replace(' .', '.').replace(' ,', ',').replace(' ?', '?').replace(' !', '!')\n",
    "\n",
    "# --- Main Execution (Final Model) ---\n",
    "MODEL_DIR = \"../3.model/Markov_chain_v0.4\"\n",
    "VOCAB_FILEPATH = os.path.join(MODEL_DIR, \"vocabulary4Markov_chain_v0.4.json\")\n",
    "WINDOW_SIZE = 3 # You can experiment by changing this value (e.g., 2, 3, 4)\n",
    "MODEL_FILENAME = f\"int_model_ws{WINDOW_SIZE}.json\"\n",
    "MODEL_FILEPATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "# 1. Build or Load Vocabulary\n",
    "if os.path.exists(VOCAB_FILEPATH):\n",
    "    print(\"Loading existing vocabulary...\")\n",
    "    word_to_int, int_to_word = load_vocab(VOCAB_FILEPATH)\n",
    "    # Tokenize the text again for model building, even with a loaded vocab\n",
    "    processed_lines, _, _ = tokenize_and_build_vocab(train_data)\n",
    "else:\n",
    "    print(\"Building and saving new vocabulary...\")\n",
    "    processed_lines, word_to_int, int_to_word = tokenize_and_build_vocab(train_data)\n",
    "    save_vocab(word_to_int, int_to_word, VOCAB_FILEPATH)\n",
    "\n",
    "# 2. Build or Load the Integer Model\n",
    "if os.path.exists(MODEL_FILEPATH):\n",
    "    print(f\"Loading final model with window size {WINDOW_SIZE}...\")\n",
    "    # The generic `load_model_window` works, but we'll call a dedicated one for clarity if needed.\n",
    "    # Note: `load_model_window` from the previous cell should be available or redefined here.\n",
    "    final_model = load_model_window(MODEL_FILEPATH)\n",
    "else:\n",
    "    print(f\"Training final model with window size {WINDOW_SIZE}...\")\n",
    "    final_model_tuples = build_integer_model(processed_lines, word_to_int, window_size=WINDOW_SIZE)\n",
    "    # Save the model using the windowed saver which handles tuple keys\n",
    "    save_model_window(final_model_tuples, MODEL_FILEPATH)\n",
    "    # For generation, we need the string-keyed version that load_model_window provides\n",
    "    final_model = {str(k): v for k, v in final_model_tuples.items()}\n",
    "\n",
    "# 3. Generate Text\n",
    "generated_text = generate_text_integer_model(\n",
    "    final_model,\n",
    "    int_to_word,\n",
    "    word_to_int,\n",
    "    length=75,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    start_seed=\"the history of\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Text Generated from Final Model ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Conceptual Limit and the Path Forward\n",
    "\n",
    "After implementing a series of powerful optimizations—memory-efficient processing, increased context windows, grammar-aware tokenization, and integer encoding—we have pushed the Markov chain architecture to its logical peak. This is the point where we must acknowledge the fundamental limitations of the model itself.\n",
    "\n",
    "### The Inescapable Roadblock: Statistics vs. Semantics\n",
    "\n",
    "Our model has become an incredibly sophisticated pattern-matching engine. It knows which words are statistically likely to follow other sequences of words. However, it has no ability to grasp the actual **meaning**, or **semantics**, of the words it is processing.\n",
    "\n",
    "To the model, the words \"king,\" \"queen,\" and \"cabbage\" are just unique integers. It does not understand that two of those words relate to royalty and one relates to vegetables. It only knows the statistical probabilities of their arrangement. This is the core conceptual roadblock of the Markov assumption: **it can mimic structure, but it cannot comprehend meaning**.\n",
    "\n",
    "### Final Refinements vs. New Architectures\n",
    "\n",
    "This is where most of the *architectural* optimizations for a Markov chain end. While we have reached the limit of what this type of model can conceptually do, there are still two key refinements we could make. These wouldn't change the fundamental nature of the model, but they would improve its performance and robustness.\n",
    "\n",
    "#### 1. Performance Optimization: Sparse Matrices\n",
    "\n",
    "* **What:** Instead of a nested Python dictionary, we could represent our model as a **sparse matrix** using libraries like `scipy.sparse`. The rows would represent the state (the tuple of previous words), the columns would represent the next possible word, and the cell value would be the transition count or probability.\n",
    "* **Why:**\n",
    "    * **Speed:** Mathematical operations on these matrices (like calculating probabilities) are performed using highly optimized C code (a process called vectorization), which is orders of magnitude faster than looping through a Python dictionary.\n",
    "    * **Memory:** While our integer-encoded dictionary is very good, a sparse matrix has even less memory overhead per entry, making it the superior choice for extremely large vocabularies.\n",
    "\n",
    "#### 2. Quality Optimization: Smoothing and Backoff\n",
    "\n",
    "* **What:** What happens if the model generates a sequence of words it has never seen before in the training data? It will have no entry for this state and will crash or stop generating text. **Smoothing** (like Laplace smoothing) is a technique that gives a tiny, non-zero probability to every *possible* transition, even unseen ones. A more advanced technique, **Backoff**, would make the model \"back off\" to a smaller context window (e.g., if it can't find a match for a 3-word sequence, it tries matching on the last 2 words) to find a next step.\n",
    "* **Why:** This makes the model more **robust**. It prevents it from getting stuck on unfamiliar phrases and allows it to generate longer, more fluent sequences without failing, which directly improves the quality and reliability of the output.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While these refinements would make our model faster and more resilient, they do not solve the core problem of semantic understanding. To achieve a more human-like grasp of language—one that understands context, topics, and long-range dependencies—we would need to move beyond Markov chains to a different class of models entirely: **neural networks**, such as Recurrent Neural Networks (LSTMs) or the state-of-the-art Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
