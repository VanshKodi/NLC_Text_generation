 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQbkSYL8DhPE"
   },
   "source": [
    "## Our Development Philosophy: From Baseline to Optimized\n",
    "\n",
    "This project follows a deliberate and iterative approach to model development. Our core principle is to ensure that every change is measured, understood, and contributes positively to the final result. This methodology transforms the development process from a series of guesses into a scientific experiment.\n",
    "\n",
    "### Phase 1: Build the Brute-Force Baseline\n",
    "\n",
    "First, we create the simplest possible, end-to-end working model. This **bare-minimum** version is not intended to be sophisticated; its purpose is to be a functional starting point that serves two critical purposes:\n",
    "\n",
    "1.  **Proof of Concept:** It confirms that our data pipeline, from loading to processing and modeling, is functional. It validates our foundational assumptions.\n",
    "2.  **Establish a Benchmark:** It provides a clear **baseline performance metric**. All future work will be measured against this initial score. Without a baseline, it's impossible to quantify improvement.\n",
    "\n",
    "### Phase 2: Optimize One Step at a Time\n",
    "\n",
    "Once the baseline is established, we begin a cycle of incremental improvement. We strictly adhere to the principle of making **one isolated change at a time**. This is the most critical aspect of our philosophy.\n",
    "\n",
    "Instead of overhauling the model at once and being unable to pinpoint the source of a change, we will:\n",
    "\n",
    "* **Target** a single component for improvement (e.g., memory usage, context awareness, grammar).\n",
    "* **Implement** that one specific change.\n",
    "* **Measure** its exact impact on performance and resource consumption.\n",
    "\n",
    "This methodical process removes guesswork and allows us to attribute performance gains or losses directly to a specific action, ensuring that every step forward is a confident one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading the Dataset\n",
    "\n",
    "Our journey begins by loading the `wikitext` dataset, a large corpus of text derived from high-quality Wikipedia articles. We use the `datasets` library from Hugging Face for this task, which provides a standardized and efficient way to handle large datasets.\n",
    "\n",
    "We also specify a custom cache directory (`../2.data`) to ensure that the downloaded data is stored in a predictable location within our project structure, making it easier to manage and reuse without re-downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSS3p4etDy-C"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define a custom directory to cache the downloaded dataset\n",
    "my_custom_cache_dir = \"../2.data\"\n",
    "\n",
    "# Load the 'wikitext-2-raw-v1' version of the wikitext dataset\n",
    "wikitext_dataset = load_dataset(\n",
    "    \"wikitext\",\n",
    "    \"wikitext-2-raw-v1\",\n",
    "    cache_dir=my_custom_cache_dir\n",
    ")\n",
    "\n",
    "# Print the dataset structure to see the different splits (train, validation, test)\n",
    "print(wikitext_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initial Data Exploration\n",
    "\n",
    "Before we can clean the data, we need to understand its raw form. We'll inspect the first few lines of the training set to identify patterns, noise, and artifacts that need to be addressed in our preprocessing stage. This step is crucial for formulating an effective cleaning strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7eMZxqOFVyA"
   },
   "outputs": [],
   "source": [
    "# Create separate variables for easier access to each dataset split\n",
    "test_data = wikitext_dataset['test']\n",
    "train_data = wikitext_dataset['train']\n",
    "validation_data = wikitext_dataset['validation']\n",
    "\n",
    "# Print the first 25 lines of the training data to inspect its content\n",
    "for i in range(25):\n",
    "  print(f\"{i}:{train_data[i]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKOkr78xHxzu"
   },
   "source": [
    "### Step 3: Defining the Preprocessing Strategy\n",
    "\n",
    "Based on our initial exploration, we have identified several types of noise in the raw text. To prepare the data for our model, we will implement a multi-step cleaning process:\n",
    "\n",
    "* **Remove Article Headings:** The dataset uses lines starting with ` = ` to denote section titles (e.g., `= = Gameplay = =`). These are metadata, not narrative text, and should be removed.\n",
    "* **Handle Special Characters:** The text contains artifacts like `@-@` and non-ASCII characters (e.g., `戦場のヴァルキュリア3`). We will remove these to create a cleaner, more consistent vocabulary.\n",
    "* **Standardize Casing:** All text will be converted to lowercase to ensure that the model treats the same word (e.g., \"The\" and \"the\") as a single entity.\n",
    "* **Remove Empty Lines:** The dataset contains many blank lines, which add no value and should be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Measuring the Impact of Preprocessing\n",
    "\n",
    "Before applying our cleaning functions, we will establish a baseline measurement of the dataset's size. We'll calculate the total number of rows and characters in each split (train, validation, and test). After preprocessing, we will perform the same calculation again. This allows us to quantify the exact impact of our cleaning process, showing how much noise we have successfully removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "li5oyf7PGZ47"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# --- Before Preprocessing ---\n",
    "print(\"--- Before Cleaning ---\")\n",
    "# Calculate and print the initial size of each dataset split\n",
    "train_chars_before = sum(len(line) for line in train_data['text'] if line)\n",
    "validation_chars_before = sum(len(line) for line in validation_data['text'] if line)\n",
    "test_chars_before = sum(len(line) for line in test_data['text'] if line)\n",
    "print(f\"Training Data:   {train_data.num_rows:,} rows, {train_chars_before:,} characters\")\n",
    "print(f\"Validation Data: {validation_data.num_rows:,} rows, {validation_chars_before:,} characters\")\n",
    "print(f\"Test Data:       {test_data.num_rows:,} rows, {test_chars_before:,} characters\")\n",
    "\n",
    "# --- Preprocessing Steps ---\n",
    "# The 'datasets' library allows us to chain operations for a clean pipeline.\n",
    "\n",
    "# 1. Filter out article headings\n",
    "processed_dataset = wikitext_dataset.filter(\n",
    "    lambda example: not example['text'].strip().startswith(' = ')\n",
    ")\n",
    "\n",
    "# 2. Apply text cleaning and normalization to each entry\n",
    "processed_dataset = processed_dataset.map(\n",
    "    lambda example: {\n",
    "        'text': re.sub(\n",
    "            r'[^a-zA-Z0-9\\s.,\\'?!-]', '', # Keep only alphanumeric, common punctuation, and whitespace\n",
    "            example['text'].lower().replace('@-@', '') # Convert to lowercase and remove artifacts\n",
    "        ).strip()\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Filter out any lines that became empty after cleaning\n",
    "processed_dataset = processed_dataset.filter(\n",
    "    lambda example: len(example['text']) > 0\n",
    ")\n",
    "\n",
    "# --- After Preprocessing ---\n",
    "print(\"\\n--- After Cleaning ---\")\n",
    "# Re-assign the cleaned data to our variables for future use\n",
    "train_data = processed_dataset['train']\n",
    "validation_data = processed_dataset['validation']\n",
    "test_data = processed_dataset['test']\n",
    "\n",
    "# Calculate and print the final size of each dataset split\n",
    "train_chars_after = sum(len(line) for line in train_data['text'] if line)\n",
    "validation_chars_after = sum(len(line) for line in validation_data['text'] if line)\n",
    "test_chars_after = sum(len(line) for line in test_data['text'] if line)\n",
    "print(f\"Training Data:   {train_data.num_rows:,} rows, {train_chars_after:,} characters\")\n",
    "print(f\"Validation Data: {validation_data.num_rows:,} rows, {validation_chars_after:,} characters\")\n",
    "print(f\"Test Data:       {test_data.num_rows:,} rows, {test_chars_after:,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Verifying the Cleaned Data\n",
    "\n",
    "After preprocessing, it's essential to visually inspect the data again. This final check confirms that our cleaning functions worked as expected and that the text is now in a suitable format for our model. We expect to see clean, lowercase text with no strange artifacts or article headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCH8m9xVK52I"
   },
   "outputs": [],
   "source": [
    "# Visually inspect the first 20 lines of the cleaned training data\n",
    "for i in range(20):\n",
    "  print(f\"{i}:{train_data[i]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIJMdwzPN5_P"
   },
   "source": [
    "## Baseline Model: The Simple Markov Chain\n",
    "\n",
    "Our first model is a foundational, first-order Markov chain. It's a simple probabilistic model that predicts the next word in a sequence based *solely* on the current word, ignoring all previous context.\n",
    "\n",
    "### What is \"The Model\"?\n",
    "\n",
    "In this context, the \"model\" isn't a complex algorithm but a simple and intuitive data structure: a **Python dictionary**. This dictionary is the final output of the \"training\" process. It stores all the learned word-to-word transition probabilities from the input text.\n",
    "\n",
    "### The Structure of the Model (The Dictionary)\n",
    "\n",
    "The model is a nested dictionary with a specific structure: `word -> {next_word: count}`.\n",
    "\n",
    "* **Level 1: The Keys (Current Words)**\n",
    "    * The keys of the main dictionary are all the unique words found in the text. Each key represents a possible \"current state.\"\n",
    "\n",
    "* **Level 2: The Values (Next Words and Their Frequencies)**\n",
    "    * The value associated with each key is *another dictionary*.\n",
    "    * In this inner dictionary, the keys are all the words that have ever appeared immediately after the \"current word.\"\n",
    "    * The values are the counts (integers) of how many times that specific transition occurred.\n",
    "\n",
    "### A Concrete Example\n",
    "\n",
    "If our training text is: `\"the cat sat on the mat\"`\n",
    "\n",
    "The resulting model dictionary would look like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"the\": {\n",
    "        \"cat\": 1,\n",
    "        \"mat\": 1\n",
    "    },\n",
    "    \"cat\": {\n",
    "        \"sat\": 1\n",
    "    },\n",
    "    \"sat\": {\n",
    "        \"on\": 1\n",
    "    },\n",
    "    \"on\": {\n",
    "        \"the\": 1\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDXONapyNFhX"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "def build_markov_model(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Builds a simple, first-order Markov chain model from a single string of text.\n",
    "    \n",
    "    Args:\n",
    "        text: The entire training corpus as one large string.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary representing the Markov model.\n",
    "    \"\"\"\n",
    "    # Split the corpus into a list of individual words (tokens)\n",
    "    tokens = text.split()\n",
    "    model = {}\n",
    "\n",
    "    # Iterate through the tokens to build word-pair transitions\n",
    "    for i in range(len(tokens) - 1):\n",
    "        current_word = tokens[i]\n",
    "        next_word = tokens[i + 1]\n",
    "\n",
    "        # If we haven't seen the current word before, initialize it in the model\n",
    "        if current_word not in model:\n",
    "            model[current_word] = {}\n",
    "\n",
    "        # If we haven't seen this specific transition before, initialize its count\n",
    "        if next_word not in model[current_word]:\n",
    "            model[current_word][next_word] = 0\n",
    "\n",
    "        # Increment the count for this transition\n",
    "        model[current_word][next_word] += 1\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_text(model: dict, length: int = 50, start: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Generates new text using a pre-built Markov model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained Markov model dictionary.\n",
    "        length: The desired length of the generated text in words.\n",
    "        start: An optional word to begin the text generation.\n",
    "        \n",
    "    Returns:\n",
    "        A string of generated text.\n",
    "    \"\"\"\n",
    "    # If a start word isn't provided, pick one randomly from the model's known words\n",
    "    if start is None:\n",
    "      start_word = random.choice(list(model.keys()))\n",
    "    else:\n",
    "        start_word = start.lower()\n",
    "        # Check if the provided start word is in the model, otherwise start randomly\n",
    "        if start_word not in model:\n",
    "            print(f\"Warning: '{start_word}' not in model. Starting randomly.\")\n",
    "            start_word = random.choice(list(model.keys()))\n",
    "            \n",
    "    generated_text = [start_word]\n",
    "    current_word = start_word\n",
    "\n",
    "    for _ in range(length - 1):\n",
    "        # If the current word has no known following words, stop generating\n",
    "        if current_word not in model:\n",
    "            break\n",
    "\n",
    "        # Get all possible next words and their frequencies (weights)\n",
    "        next_words_dict = model[current_word]\n",
    "        possible_next_words = list(next_words_dict.keys())\n",
    "        word_frequencies = list(next_words_dict.values())\n",
    "\n",
    "        # Choose the next word based on the weighted probabilities\n",
    "        chosen_next_word = random.choices(possible_next_words, weights=word_frequencies, k=1)[0]\n",
    "\n",
    "        generated_text.append(chosen_next_word)\n",
    "        current_word = chosen_next_word\n",
    "\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# --- Helper Functions for Saving/Loading --- #\n",
    "def save_model(model, filepath):\n",
    "    \"\"\"Saves the model dictionary to a JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(model, f)\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"Loads the model dictionary from a JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# --- Main Execution (Baseline) ---\n",
    "\n",
    "model_filepath = \"./models/markov_chain_v0.1.json\"\n",
    "\n",
    "if os.path.exists(model_filepath):\n",
    "    print(\"Loading saved baseline model...\")\n",
    "    markov_model = load_model(model_filepath)\n",
    "else:\n",
    "    print(\"No saved model found. Building a new one (this may take a while)... \")\n",
    "    # This is the brute-force, memory-intensive approach\n",
    "    corpus = \" \".join(train_data['text'])\n",
    "    markov_model = build_markov_model(corpus)\n",
    "    save_model(markov_model, model_filepath)\n",
    "    print(f\"Model built and saved to {model_filepath}\")\n",
    "\n",
    "# Generate new text from the trained model\n",
    "new_text = generate_text(markov_model, length=75, start=\"morning\")\n",
    "\n",
    "print(\"\\n--- Text Generated from Baseline Model ---\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5fri8tWSehW"
   },
   "source": [
    "## Optimization 1: Memory Efficiency\n",
    "\n",
    "The initial brute-force approach required loading the entire training dataset into a **single, massive string**. This is highly problematic for large datasets for two main reasons:\n",
    "\n",
    "1.  **High Memory Usage:** It can consume several gigabytes of RAM, potentially crashing the program with a `MemoryError` on systems with limited memory.\n",
    "2.  **Loss of Context:** It destroys natural sentence boundaries, leading to a lower-quality model that can't learn to start or end sentences properly.\n",
    "\n",
    "To solve this, we **process the data incrementally**. Instead of creating one large object in memory, the optimized approach is to build the model **line-by-line**.\n",
    "\n",
    "We iterate through the dataset, processing only one line at a time to update our model dictionary. This is a highly **memory-efficient** solution, as it allows us to train on a dataset of virtually any size while using a nearly constant amount of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcuTb1jaOoc3"
   },
   "outputs": [],
   "source": [
    "def build_markov_model_incrementally(dataset_split):\n",
    "    \"\"\"\n",
    "    Builds a model by iterating through the dataset line by line (memory-efficient).\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    # Process one line at a time instead of the whole corpus at once\n",
    "    for line in dataset_split['text']:\n",
    "        tokens = line.split()\n",
    "        # Skip empty lines\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        # The core logic remains the same, but applied to smaller chunks\n",
    "        for i in range(len(tokens) - 1):\n",
    "            current_word = tokens[i]\n",
    "            next_word = tokens[i + 1]\n",
    "\n",
    "            if current_word not in model:\n",
    "                model[current_word] = {}\n",
    "\n",
    "            if next_word not in model[current_word]:\n",
    "                model[current_word][next_word] = 0\n",
    "\n",
    "            model[current_word][next_word] += 1\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Main Execution (Incremental Approach) ---\n",
    "MODEL_FILEPATH = \"./models/markov_chain_v0.2.json\"\n",
    "\n",
    "if os.path.exists(MODEL_FILEPATH):\n",
    "    print(\"Loading existing memory-efficient model...\")\n",
    "    markov_model_mem_opt = load_model(MODEL_FILEPATH)\n",
    "else:\n",
    "    print(\"No existing model found. Building a new one incrementally...\")\n",
    "    # Pass the dataset object directly to the incremental builder\n",
    "    markov_model_mem_opt = build_markov_model_incrementally(train_data)\n",
    "    save_model(markov_model_mem_opt, MODEL_FILEPATH)\n",
    "    print(f\"Model built and saved to {MODEL_FILEPATH}\")\n",
    "\n",
    "\n",
    "# Generate and Print Text\n",
    "generated_text = generate_text(\n",
    "    markov_model_mem_opt,\n",
    "    length=75,\n",
    "    start=\"morning\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Generated Text (Memory-Efficient Model) ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9Ng2W3AUw3G"
   },
   "source": [
    "## Optimization 2: Increasing Context with a Larger Window\n",
    "\n",
    "The most significant weakness of our current model is its extremely limited memory. It is a **first-order Markov chain**, meaning it only knows the previous **1** word. This leads to a critical failure in logical consistency.\n",
    "\n",
    "### The Problem of Lost Context\n",
    "\n",
    "Consider the phrase: `\"The sky is...\"`\n",
    "\n",
    "Our model's decision for the next word is based *only* on the word `\"is\"`. It has forgotten the crucial context word `\"sky\"`. The model's pool of possible next words is created from *every single instance* where `\"is\"` appeared in the training text, regardless of what came before it. It learns from unrelated phrases like:\n",
    "\n",
    "* `\"...the sky is blue...\"`\n",
    "* `\"...the company's logo is an apple...\"`\n",
    "\n",
    "When the model's only context is `\"is\"`, it blends these unrelated scenarios. It might correctly identify `\"blue\"` as probable, but `\"apple\"` remains a possibility because the model has no memory of the preceding word, `\"sky\"`.\n",
    "\n",
    "### The Solution: Using Tuples for State\n",
    "\n",
    "To solve this, we increase the model's memory by using a **tuple of multiple words** as the key. This tuple acts as our new \"state,\" effectively filtering out irrelevant choices.\n",
    "\n",
    "* **Old Model State (Plain Word):** `is`\n",
    "    * Possible Next Words: `{ \"apple\": 3, \"blue\": 4, \"green\": 2, ... }`\n",
    "\n",
    "* **New Model State (Tuple):** `('sky', 'is')`\n",
    "    * Possible Next Words: `{ \"blue\": 150, \"clear\": 80, \"overcast\": 45, ... }`\n",
    "\n",
    "By using a tuple, the model's state is now `('sky', 'is')`. The pool of possible next words it considers is now completely different and far more relevant. The word `\"apple\"` is unlikely to even be in this new list, leading to much more coherent and context-aware text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RMaeZZYUIbO"
   },
   "outputs": [],
   "source": [
    "import ast # Used to safely convert string representations of tuples back to tuples\n",
    "\n",
    "def build_markov_model_with_window(dataset_split, window_size=2):\n",
    "    \"\"\"\n",
    "    Builds a higher-order Markov model using a tuple of words (the \"window\") as the state.\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    for line in dataset_split['text']:\n",
    "        tokens = line.split()\n",
    "        # Ensure the line is long enough to create at least one windowed state\n",
    "        if len(tokens) < window_size + 1:\n",
    "            continue\n",
    "        # Iterate through the tokens, creating states of size `window_size`\n",
    "        for i in range(len(tokens) - window_size):\n",
    "            current_state = tuple(tokens[i : i + window_size])\n",
    "            next_word = tokens[i + window_size]\n",
    "            \n",
    "            if current_state not in model:\n",
    "                model[current_state] = {}\n",
    "            if next_word not in model[current_state]:\n",
    "                model[current_state][next_word] = 0\n",
    "            model[current_state][next_word] += 1\n",
    "    return model\n",
    "\n",
    "def save_model_window(model, filepath):\n",
    "    \"\"\"\n",
    "    Saves the windowed model to JSON, converting tuple keys to strings.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    # JSON does not support tuple keys, so we convert them to strings for serialization\n",
    "    string_keyed_model = {str(key): value for key, value in model.items()}\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(string_keyed_model, f)\n",
    "\n",
    "def load_model_window(filepath):\n",
    "    \"\"\"\n",
    "    Loads a windowed model from JSON, converting string keys back to tuples.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        string_keyed_model = json.load(f)\n",
    "        # `ast.literal_eval` safely converts the string representation of the tuple back to a tuple object\n",
    "        model = {ast.literal_eval(key): value for key, value in string_keyed_model.items()}\n",
    "    return model\n",
    "\n",
    "def generate_text_with_window(model, length=75, window_size=2, start_seed=None):\n",
    "    \"\"\"\n",
    "    Generates text from a windowed model, optionally starting with a given seed phrase.\n",
    "    \"\"\"\n",
    "    start_state = None\n",
    "\n",
    "    if start_seed:\n",
    "        seed_words = start_seed.lower().split()\n",
    "        if len(seed_words) != window_size:\n",
    "            print(f\"Warning: Seed phrase has {len(seed_words)} words, but model expects {window_size}. Starting randomly.\")\n",
    "        else:\n",
    "            potential_start_state = tuple(seed_words)\n",
    "            if potential_start_state in model:\n",
    "                start_state = potential_start_state\n",
    "            else:\n",
    "                print(f\"Warning: Seed phrase '{start_seed}' not found in model. Starting randomly.\")\n",
    "\n",
    "    # If no valid seed is found, pick a random starting state from the model\n",
    "    if start_state is None:\n",
    "        start_state = random.choice(list(model.keys()))\n",
    "\n",
    "    generated_text = list(start_state)\n",
    "    current_state = start_state\n",
    "\n",
    "    for _ in range(length - window_size):\n",
    "        if current_state not in model:\n",
    "            # If we encounter a state that has no known next word, end generation\n",
    "            break\n",
    "            \n",
    "        next_words_dict = model[current_state]\n",
    "        possible_next_words = list(next_words_dict.keys())\n",
    "        word_frequencies = list(next_words_dict.values())\n",
    "        \n",
    "        chosen_next_word = random.choices(possible_next_words, weights=word_frequencies, k=1)[0]\n",
    "        generated_text.append(chosen_next_word)\n",
    "        \n",
    "        # Update the current state to be the last `window_size` words\n",
    "        current_state = tuple(generated_text[-window_size:])\n",
    "\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# --- Main Execution (Windowed Approach) ---\n",
    "WINDOW_SIZE = 2\n",
    "model_dir = \"./models\"\n",
    "model_filename = f\"markov_model_ws{WINDOW_SIZE}.json\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "\n",
    "if os.path.exists(model_filepath):\n",
    "    print(f\"Loading model with window size {WINDOW_SIZE}...\")\n",
    "    windowed_markov_model = load_model_window(model_filepath)\n",
    "else:\n",
    "    print(f\"Saved model not found. Training a new one with window size {WINDOW_SIZE}...\")\n",
    "    windowed_markov_model = build_markov_model_with_window(train_data, window_size=WINDOW_SIZE)\n",
    "    save_model_window(windowed_markov_model, model_filepath)\n",
    "    print(f\"Model saved to {model_filepath}\")\n",
    "\n",
    "# Generate text from the loaded/trained model\n",
    "new_text = generate_text_with_window(\n",
    "    windowed_markov_model,\n",
    "    length=75,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    start_seed=\"the sky\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Text Generated from Windowed Model ---\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t62CRbubcNTj"
   },
   "source": [
    "## Final Optimizations: Grammar and Memory\n",
    "\n",
    "In the final version, we will perform two significant optimizations simultaneously to improve both the quality of the generated text and the model's efficiency.\n",
    "\n",
    "### 1. Teaching the Model Sentence Structure\n",
    "\n",
    "Currently, our model has no sense of the start and end of a sentence. A word at the end of a sentence and the same word in the middle are treated as completely different because of the attached punctuation.\n",
    "\n",
    "> For example, `\"found\"` and `\"found.\"` are two entirely separate words for our model.\n",
    "\n",
    "This is a major flaw, as the model never learns that a period signifies the end of a thought. We solve this by **treating punctuation as its own word (or \"token\")**. During preprocessing, we will pad spaces around major punctuation marks.\n",
    "\n",
    "* **Before:** `\"the cat sat found.\"`\n",
    "* **After:** The text is tokenized into `[\"the\", \"cat\", \"sat\", \"found\", \".\"]`\n",
    "\n",
    "This teaches the model the crucial relationship between words and punctuation, allowing it to learn how to properly end the sentences it generates.\n",
    "\n",
    "### 2. Reducing Memory Footprint with Integer Encoding\n",
    "\n",
    "Storing the entire model using full words (strings) is inefficient. Typically, storing one character requires one byte of memory, so a 7-character word like `\"awesome\"` uses at least 7 bytes. For a large corpus, the memory required for the model dictionary can become enormous.\n",
    "\n",
    "To solve this, we create a simple form of **tokenization** by mapping each unique word in our vocabulary to a unique integer.\n",
    "\n",
    "> For example: `{\"a\": 1, \"the\": 14, \"awesome\": 56, ...}`\n",
    "\n",
    "This fundamentally changes our model. Instead of a single large dictionary, our saved model now consists of **two essential components**:\n",
    "\n",
    "1.  **The Vocabulary Maps:** Two dictionaries, one that maps words to their unique integer IDs (`word_to_int`) and another that maps IDs back to words (`int_to_word`) for generation.\n",
    "2.  **The Core Model:** The main dictionary, which now stores these lightweight integers instead of heavy strings, dramatically reducing its size in memory and on disk.\n",
    "\n",
    "### 3. Decoupling Vocabulary from the Model for Experimentation\n",
    "\n",
    "Since we want to check the performance of our models for different window sizes, it is inefficient to store the vocabulary inside each model file. The vocabulary of the training text is constant; it does not change whether we use a window size of 2, 3, or 4.\n",
    "\n",
    "Therefore, we will adopt a more organized storage strategy:\n",
    "\n",
    "* **Vocabulary (`vocabulary.json`):** The word-to-integer mappings will be built once from the training data and saved to their own separate file.\n",
    "* **Model Dictionaries (`int_model_ws2.json`, etc.):** Each model, trained with a specific window size, will be saved to its own file, named dynamically based on its configuration. These model files will only contain the integer-based transition logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un0u_aaSZ-sy"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import ast\n",
    "\n",
    "# --- Step 1: Grammar-Aware Preprocessing and Vocabulary Building ---\n",
    "\n",
    "def tokenize_and_build_vocab(dataset_split):\n",
    "    \"\"\"\n",
    "    Processes text to separate punctuation and builds word-to-integer mappings.\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    processed_lines = []\n",
    "\n",
    "    for line in dataset_split['text']:\n",
    "        # Add spaces around punctuation to treat them as separate tokens\n",
    "        line = re.sub(r'([.,?!])', r' \\1 ', line)\n",
    "        tokens = line.split()\n",
    "        processed_lines.append(tokens)\n",
    "        for token in tokens:\n",
    "            word_counts[token] = word_counts.get(token, 0) + 1\n",
    "            \n",
    "    # Create vocabulary mappings\n",
    "    # Sort words by frequency to assign lower integers to more common words (optional but good practice)\n",
    "    sorted_words = sorted(word_counts.keys(), key=lambda x: word_counts[x], reverse=True)\n",
    "    word_to_int = {word: i for i, word in enumerate(sorted_words)}\n",
    "    int_to_word = {i: word for i, word in enumerate(sorted_words)}\n",
    "    \n",
    "    return processed_lines, word_to_int, int_to_word\n",
    "\n",
    "def save_vocab(word_to_int, int_to_word, filepath):\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\"word_to_int\": word_to_int, \"int_to_word\": int_to_word}, f)\n",
    "\n",
    "def load_vocab(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data['word_to_int'], data['int_to_word']\n",
    "\n",
    "# --- Step 2: Build the Integer-Based Markov Model ---\n",
    "\n",
    "def build_integer_model(processed_lines, word_to_int, window_size=2):\n",
    "    \"\"\"\n",
    "    Builds a windowed Markov model using integer IDs instead of strings.\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    for tokens in processed_lines:\n",
    "        if len(tokens) < window_size + 1:\n",
    "            continue\n",
    "        # Convert the list of word tokens to a list of integer IDs\n",
    "        int_tokens = [word_to_int[token] for token in tokens if token in word_to_int]\n",
    "        \n",
    "        for i in range(len(int_tokens) - window_size):\n",
    "            current_state = tuple(int_tokens[i : i + window_size])\n",
    "            next_word_id = int_tokens[i + window_size]\n",
    "            \n",
    "            if current_state not in model:\n",
    "                model[current_state] = {}\n",
    "            if next_word_id not in model[current_state]:\n",
    "                model[current_state][next_word_id] = 0\n",
    "            model[current_state][next_word_id] += 1\n",
    "    return model\n",
    "\n",
    "# --- Step 3: Text Generation with Integer Model ---\n",
    "\n",
    "def generate_text_integer_model(model, int_to_word, word_to_int, length=75, window_size=2, start_seed=None):\n",
    "    \"\"\"\n",
    "    Generates text from an integer-based model, then decodes it back to words.\n",
    "    \"\"\"\n",
    "    start_state = None\n",
    "    if start_seed:\n",
    "        seed_words = start_seed.lower().split()\n",
    "        if len(seed_words) == window_size and all(word in word_to_int for word in seed_words):\n",
    "            potential_start_state = tuple(word_to_int[word] for word in seed_words)\n",
    "            if potential_start_state in model:\n",
    "                start_state = potential_start_state\n",
    "\n",
    "    if start_state is None:\n",
    "        start_state = random.choice(list(model.keys()))\n",
    "\n",
    "    generated_ids = list(start_state)\n",
    "    current_state = start_state\n",
    "\n",
    "    for _ in range(length - window_size):\n",
    "        if current_state not in model:\n",
    "            break\n",
    "        \n",
    "        next_ids_dict = model[current_state]\n",
    "        possible_next_ids = list(next_ids_dict.keys())\n",
    "        id_frequencies = list(next_ids_dict.values())\n",
    "        \n",
    "        # Note: The keys (possible_next_ids) are strings from JSON, so we convert to int\n",
    "        chosen_next_id = random.choices([int(i) for i in possible_next_ids], weights=id_frequencies, k=1)[0]\n",
    "        generated_ids.append(chosen_next_id)\n",
    "        current_state = tuple(generated_ids[-window_size:])\n",
    "\n",
    "    # Decode the sequence of IDs back into words\n",
    "    generated_words = [int_to_word.get(str(id), '?') for id in generated_ids]\n",
    "    \n",
    "    # Post-process the text for better readability (joining punctuation correctly)\n",
    "    return ' '.join(generated_words).replace(' .', '.').replace(' ,', ',').replace(' ?', '?').replace(' !', '!')\n",
    "\n",
    "# --- Main Execution (Final Model) ---\n",
    "MODEL_DIR = \"./models\"\n",
    "VOCAB_FILEPATH = os.path.join(MODEL_DIR, \"vocabulary.json\")\n",
    "WINDOW_SIZE = 3\n",
    "MODEL_FILENAME = f\"int_model_ws{WINDOW_SIZE}.json\"\n",
    "MODEL_FILEPATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "# 1. Build or Load Vocabulary\n",
    "if os.path.exists(VOCAB_FILEPATH):\n",
    "    print(\"Loading existing vocabulary...\")\n",
    "    word_to_int, int_to_word = load_vocab(VOCAB_FILEPATH)\n",
    "    # We still need to tokenize the text for model building, even with a loaded vocab\n",
    "    processed_lines, _, _ = tokenize_and_build_vocab(train_data)\n",
    "else:\n",
    "    print(\"Building and saving new vocabulary...\")\n",
    "    processed_lines, word_to_int, int_to_word = tokenize_and_build_vocab(train_data)\n",
    "    save_vocab(word_to_int, int_to_word, VOCAB_FILEPATH)\n",
    "\n",
    "# 2. Build or Load the Integer Model\n",
    "if os.path.exists(MODEL_FILEPATH):\n",
    "    print(f\"Loading final model with window size {WINDOW_SIZE}...\")\n",
    "    final_model = load_model_window(MODEL_FILEPATH)\n",
    "else:\n",
    "    print(f\"Training final model with window size {WINDOW_SIZE}...\")\n",
    "    final_model = build_integer_model(processed_lines, word_to_int, window_size=WINDOW_SIZE)\n",
    "    save_model_window(final_model, MODEL_FILEPATH)\n",
    "\n",
    "# 3. Generate Text\n",
    "generated_text = generate_text_integer_model(\n",
    "    final_model,\n",
    "    int_to_word,\n",
    "    word_to_int,\n",
    "    length=75,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    start_seed=\"the history of\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Text Generated from Final Model ---\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
